{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9393ade5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c460d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "id": "d02ff7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_predict_final(X_train, X_test, y_train, y_test, test_final, submission_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Evaluates models on train/test split.\n",
    "    Uses best model to predict on separate test_final (no target).\n",
    "    test_final must include Item_Identifier & Outlet_Identifier.\n",
    "    Saves predictions to CSV.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "    import pandas as pd\n",
    "\n",
    "    models = {\n",
    "        'LinearRegression': LinearRegression(),\n",
    "        'Ridge': Ridge(),\n",
    "        'Lasso': Lasso(),\n",
    "        'DecisionTree': DecisionTreeRegressor(random_state=42),\n",
    "        'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor( random_state=42),\n",
    "        'MLPRegressor': MLPRegressor(hidden_layer_sizes=(1024, 512, 256, 128), max_iter=500, random_state=42)\n",
    "    }\n",
    "\n",
    "    results = []\n",
    "    model_store = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MAE': round(mean_absolute_error(y_test, preds), 2),\n",
    "            'MAPE': round(mean_absolute_percentage_error(y_test, preds) * 100, 2),\n",
    "            'RMSE': round(rmse, 2),\n",
    "            'R2 Score': round(r2_score(y_test, preds), 4)\n",
    "        })\n",
    "\n",
    "        model_store[name] = model\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_model_name = results_df.sort_values(by='RMSE').iloc[0]['Model']\n",
    "    best_model = model_store[best_model_name]\n",
    "\n",
    "    print(f\"‚úÖ Best model: {best_model_name} (RMSE: {results_df.loc[results_df['Model'] == best_model_name, 'RMSE'].values[0]})\")\n",
    "\n",
    "    # Drop ID cols before prediction\n",
    "\n",
    "    test_preds = best_model.predict(test_final)\n",
    "\n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        'Item_Identifier': test['Item_Identifier'],\n",
    "        'Outlet_Identifier': test['Outlet_Identifier'],\n",
    "        'Item_Outlet_Sales': np.clip(test_preds, 0, None)\n",
    "    })\n",
    "\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"üìÅ Submission saved to: {submission_file}\")\n",
    "\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "id": "aed6fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final=test[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "id": "92fd09c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best model: MLPRegressor (RMSE: 1067.28)\n",
      "üìÅ Submission saved to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_and_predict_final(X_train, X_test, y_train, y_test, test_final=test_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "id": "97d99e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>Model</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>870.56</td>\n",
       "      <td>103.86</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.5052</td>\n",
       "      <td>1168.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>870.46</td>\n",
       "      <td>103.81</td>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.5052</td>\n",
       "      <td>1168.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>870.34</td>\n",
       "      <td>103.62</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.5054</td>\n",
       "      <td>1168.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007.43</td>\n",
       "      <td>70.94</td>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>1444.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>763.88</td>\n",
       "      <td>58.71</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>1098.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>841.90</td>\n",
       "      <td>69.25</td>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.4789</td>\n",
       "      <td>1199.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>748.59</td>\n",
       "      <td>62.47</td>\n",
       "      <td>MLPRegressor</td>\n",
       "      <td>0.5871</td>\n",
       "      <td>1067.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       MAE    MAPE             Model  R2 Score     RMSE\n",
       "0   870.56  103.86  LinearRegression    0.5052  1168.38\n",
       "1   870.46  103.81             Ridge    0.5052  1168.35\n",
       "2   870.34  103.62             Lasso    0.5054  1168.21\n",
       "3  1007.43   70.94      DecisionTree    0.2433  1444.88\n",
       "4   763.88   58.71      RandomForest    0.5625  1098.68\n",
       "5   841.90   69.25  GradientBoosting    0.4789  1199.08\n",
       "6   748.59   62.47      MLPRegressor    0.5871  1067.28"
      ]
     },
     "execution_count": 1133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e48c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "933bb043",
   "metadata": {},
   "source": [
    "## 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a1f4ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 2048), nn.ReLU(),\n",
    "            nn.Linear(2048, 1024), nn.ReLU(),\n",
    "            nn.Linear(1024, 512), nn.ReLU(),\n",
    "            nn.Linear(512, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 128), nn.ReLU(),\n",
    "            nn.Linear(128, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def evaluate_and_predict_final(X_train, X_test, y_train, y_test, test_final, test_ids, submission_file=\"submission.csv\"):\n",
    "    \"\"\"\n",
    "    Trains PyTorch MLP on X_train/y_train, evaluates on X_test/y_test.\n",
    "    Predicts on test_final and saves predictions to CSV with test_ids.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure numpy\n",
    "    X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    X_test = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "    y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "    y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "    test_final = test_final.values if isinstance(test_final, pd.DataFrame) else test_final\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    model = TorchMLP(X_train.shape[1]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print(\"üî• Training PyTorch MLP...\")\n",
    "    for epoch in range(1, 2000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_test_tensor).cpu().numpy().flatten()\n",
    "                val_true = y_test_tensor.cpu().numpy().flatten()\n",
    "                rmse = mean_squared_error(val_true, val_preds, squared=False)\n",
    "                r2 = r2_score(val_true, val_preds)\n",
    "                mape = mean_absolute_percentage_error(val_true, val_preds) * 100\n",
    "\n",
    "            print(f\"Epoch {epoch:03d} | Loss: {loss.item():.4f} | RMSE: {rmse:.2f} | R¬≤: {r2:.4f} | MAPE: {mape:.2f}%\")\n",
    "\n",
    "    # Prediction on test_final\n",
    "    model.eval()\n",
    "    X_final_tensor = torch.tensor(test_final, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        final_preds = model(X_final_tensor).cpu().numpy().flatten()\n",
    "        final_preds = np.clip(final_preds, 0, None)\n",
    "\n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        'Item_Identifier': test_ids['Item_Identifier'].values,\n",
    "        'Outlet_Identifier': test_ids['Outlet_Identifier'].values,\n",
    "        'Item_Outlet_Sales': final_preds\n",
    "    })\n",
    "\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"‚úÖ Submission saved to: {submission_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "17ebb427",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Training PyTorch MLP...\n",
      "Epoch 001 | Loss: 6284672.0000 | RMSE: 2530.58 | R¬≤: -1.8935 | MAPE: 99.85%\n",
      "Epoch 010 | Loss: 3340672.5000 | RMSE: 1512.85 | R¬≤: -0.0341 | MAPE: 93.44%\n",
      "Epoch 020 | Loss: 2068844.1250 | RMSE: 1456.12 | R¬≤: 0.0420 | MAPE: 96.94%\n",
      "Epoch 030 | Loss: 1646004.5000 | RMSE: 1233.25 | R¬≤: 0.3128 | MAPE: 142.88%\n",
      "Epoch 040 | Loss: 1529270.1250 | RMSE: 1236.87 | R¬≤: 0.3088 | MAPE: 145.91%\n",
      "Epoch 050 | Loss: 1542834.8750 | RMSE: 1233.56 | R¬≤: 0.3124 | MAPE: 126.95%\n",
      "Epoch 060 | Loss: 1514433.5000 | RMSE: 1228.96 | R¬≤: 0.3176 | MAPE: 128.25%\n",
      "Epoch 070 | Loss: 1503066.1250 | RMSE: 1220.29 | R¬≤: 0.3272 | MAPE: 133.60%\n",
      "Epoch 080 | Loss: 1494068.6250 | RMSE: 1216.64 | R¬≤: 0.3312 | MAPE: 135.42%\n",
      "Epoch 090 | Loss: 1482726.3750 | RMSE: 1213.24 | R¬≤: 0.3349 | MAPE: 135.05%\n",
      "Epoch 100 | Loss: 1471952.2500 | RMSE: 1209.94 | R¬≤: 0.3385 | MAPE: 134.49%\n",
      "Epoch 110 | Loss: 1460177.6250 | RMSE: 1206.36 | R¬≤: 0.3424 | MAPE: 133.84%\n",
      "Epoch 120 | Loss: 1445644.7500 | RMSE: 1201.89 | R¬≤: 0.3473 | MAPE: 133.60%\n",
      "Epoch 130 | Loss: 1427508.7500 | RMSE: 1196.29 | R¬≤: 0.3534 | MAPE: 133.99%\n",
      "Epoch 140 | Loss: 1406370.2500 | RMSE: 1190.39 | R¬≤: 0.3597 | MAPE: 134.92%\n",
      "Epoch 150 | Loss: 1388680.1250 | RMSE: 1188.14 | R¬≤: 0.3621 | MAPE: 133.54%\n",
      "Epoch 160 | Loss: 1378092.1250 | RMSE: 1183.71 | R¬≤: 0.3669 | MAPE: 136.79%\n",
      "Epoch 170 | Loss: 1366651.1250 | RMSE: 1179.62 | R¬≤: 0.3713 | MAPE: 134.43%\n",
      "Epoch 180 | Loss: 1353698.5000 | RMSE: 1173.40 | R¬≤: 0.3779 | MAPE: 135.45%\n",
      "Epoch 190 | Loss: 1340900.1250 | RMSE: 1170.47 | R¬≤: 0.3810 | MAPE: 129.28%\n",
      "Epoch 200 | Loss: 1321587.3750 | RMSE: 1159.38 | R¬≤: 0.3926 | MAPE: 132.09%\n",
      "Epoch 210 | Loss: 1308665.5000 | RMSE: 1160.12 | R¬≤: 0.3919 | MAPE: 122.08%\n",
      "Epoch 220 | Loss: 1277747.0000 | RMSE: 1138.78 | R¬≤: 0.4140 | MAPE: 128.58%\n",
      "Epoch 230 | Loss: 1280114.0000 | RMSE: 1147.77 | R¬≤: 0.4048 | MAPE: 112.69%\n",
      "Epoch 240 | Loss: 1249260.7500 | RMSE: 1146.69 | R¬≤: 0.4059 | MAPE: 106.57%\n",
      "Epoch 250 | Loss: 1235153.8750 | RMSE: 1113.47 | R¬≤: 0.4398 | MAPE: 109.92%\n",
      "Epoch 260 | Loss: 1162490.3750 | RMSE: 1085.42 | R¬≤: 0.4677 | MAPE: 117.20%\n",
      "Epoch 270 | Loss: 1435357.5000 | RMSE: 1161.91 | R¬≤: 0.3900 | MAPE: 146.40%\n",
      "Epoch 280 | Loss: 1258413.2500 | RMSE: 1131.87 | R¬≤: 0.4211 | MAPE: 135.22%\n",
      "Epoch 290 | Loss: 1262391.7500 | RMSE: 1126.02 | R¬≤: 0.4271 | MAPE: 114.63%\n",
      "Epoch 300 | Loss: 1226417.1250 | RMSE: 1120.73 | R¬≤: 0.4325 | MAPE: 113.67%\n",
      "Epoch 310 | Loss: 1216079.7500 | RMSE: 1112.26 | R¬≤: 0.4410 | MAPE: 114.70%\n",
      "Epoch 320 | Loss: 1204749.3750 | RMSE: 1104.04 | R¬≤: 0.4492 | MAPE: 117.39%\n",
      "Epoch 330 | Loss: 1194832.0000 | RMSE: 1098.86 | R¬≤: 0.4544 | MAPE: 113.94%\n",
      "Epoch 340 | Loss: 1135824.3750 | RMSE: 1065.64 | R¬≤: 0.4869 | MAPE: 93.64%\n",
      "Epoch 350 | Loss: 1237372.0000 | RMSE: 1100.00 | R¬≤: 0.4533 | MAPE: 117.54%\n",
      "Epoch 360 | Loss: 1211198.7500 | RMSE: 1111.50 | R¬≤: 0.4418 | MAPE: 107.32%\n",
      "Epoch 370 | Loss: 1194342.2500 | RMSE: 1094.92 | R¬≤: 0.4583 | MAPE: 114.84%\n",
      "Epoch 380 | Loss: 1173819.6250 | RMSE: 1090.45 | R¬≤: 0.4627 | MAPE: 111.43%\n",
      "Epoch 390 | Loss: 1160966.6250 | RMSE: 1084.58 | R¬≤: 0.4685 | MAPE: 104.55%\n",
      "Epoch 400 | Loss: 1146046.0000 | RMSE: 1075.27 | R¬≤: 0.4776 | MAPE: 101.39%\n",
      "Epoch 410 | Loss: 1125409.0000 | RMSE: 1064.67 | R¬≤: 0.4878 | MAPE: 95.51%\n",
      "Epoch 420 | Loss: 1088407.0000 | RMSE: 1045.07 | R¬≤: 0.5065 | MAPE: 87.71%\n",
      "Epoch 430 | Loss: 1351667.2500 | RMSE: 1303.78 | R¬≤: 0.2319 | MAPE: 155.62%\n",
      "Epoch 440 | Loss: 1271863.0000 | RMSE: 1092.30 | R¬≤: 0.4609 | MAPE: 114.99%\n",
      "Epoch 450 | Loss: 1187448.8750 | RMSE: 1114.03 | R¬≤: 0.4392 | MAPE: 93.93%\n",
      "Epoch 460 | Loss: 1157525.5000 | RMSE: 1081.64 | R¬≤: 0.4714 | MAPE: 98.34%\n",
      "Epoch 470 | Loss: 1139240.5000 | RMSE: 1079.05 | R¬≤: 0.4739 | MAPE: 93.11%\n",
      "Epoch 480 | Loss: 1130269.8750 | RMSE: 1069.43 | R¬≤: 0.4832 | MAPE: 92.63%\n",
      "Epoch 490 | Loss: 1121729.0000 | RMSE: 1062.57 | R¬≤: 0.4898 | MAPE: 94.65%\n",
      "Epoch 500 | Loss: 1115979.5000 | RMSE: 1059.23 | R¬≤: 0.4930 | MAPE: 87.09%\n",
      "Epoch 510 | Loss: 1083746.1250 | RMSE: 1040.11 | R¬≤: 0.5112 | MAPE: 73.92%\n",
      "Epoch 520 | Loss: 1450540.3750 | RMSE: 1089.90 | R¬≤: 0.4633 | MAPE: 113.38%\n",
      "Epoch 530 | Loss: 1359748.8750 | RMSE: 1119.03 | R¬≤: 0.4342 | MAPE: 119.61%\n",
      "Epoch 540 | Loss: 1236137.5000 | RMSE: 1103.29 | R¬≤: 0.4500 | MAPE: 109.35%\n",
      "Epoch 550 | Loss: 1163654.7500 | RMSE: 1078.54 | R¬≤: 0.4744 | MAPE: 104.94%\n",
      "Epoch 560 | Loss: 1128789.3750 | RMSE: 1072.27 | R¬≤: 0.4805 | MAPE: 99.38%\n",
      "Epoch 570 | Loss: 1121212.6250 | RMSE: 1067.30 | R¬≤: 0.4853 | MAPE: 93.98%\n",
      "Epoch 580 | Loss: 1109636.8750 | RMSE: 1058.48 | R¬≤: 0.4938 | MAPE: 92.00%\n",
      "Epoch 590 | Loss: 1100749.6250 | RMSE: 1052.29 | R¬≤: 0.4997 | MAPE: 87.78%\n",
      "Epoch 600 | Loss: 1088067.3750 | RMSE: 1045.54 | R¬≤: 0.5061 | MAPE: 82.52%\n",
      "Epoch 610 | Loss: 1071884.5000 | RMSE: 1036.39 | R¬≤: 0.5147 | MAPE: 78.51%\n",
      "Epoch 620 | Loss: 1049618.8750 | RMSE: 1024.61 | R¬≤: 0.5256 | MAPE: 74.58%\n",
      "Epoch 630 | Loss: 1022995.8125 | RMSE: 1010.71 | R¬≤: 0.5384 | MAPE: 70.48%\n",
      "Epoch 640 | Loss: 1013351.8750 | RMSE: 1007.14 | R¬≤: 0.5417 | MAPE: 59.52%\n",
      "Epoch 650 | Loss: 983930.9375 | RMSE: 1003.60 | R¬≤: 0.5449 | MAPE: 55.16%\n",
      "Epoch 660 | Loss: 966352.3125 | RMSE: 990.52 | R¬≤: 0.5567 | MAPE: 56.10%\n",
      "Epoch 670 | Loss: 963046.0625 | RMSE: 994.10 | R¬≤: 0.5535 | MAPE: 54.03%\n",
      "Epoch 680 | Loss: 957550.3750 | RMSE: 982.37 | R¬≤: 0.5639 | MAPE: 63.46%\n",
      "Epoch 690 | Loss: 949133.3750 | RMSE: 976.76 | R¬≤: 0.5689 | MAPE: 60.07%\n",
      "Epoch 700 | Loss: 949526.5625 | RMSE: 974.61 | R¬≤: 0.5708 | MAPE: 56.57%\n",
      "Epoch 710 | Loss: 946304.0000 | RMSE: 973.65 | R¬≤: 0.5717 | MAPE: 57.48%\n",
      "Epoch 720 | Loss: 945177.8750 | RMSE: 973.83 | R¬≤: 0.5715 | MAPE: 56.73%\n",
      "Epoch 730 | Loss: 944678.3125 | RMSE: 973.31 | R¬≤: 0.5720 | MAPE: 56.85%\n",
      "Epoch 740 | Loss: 944444.4375 | RMSE: 973.16 | R¬≤: 0.5721 | MAPE: 57.40%\n",
      "Epoch 750 | Loss: 944401.0625 | RMSE: 973.26 | R¬≤: 0.5720 | MAPE: 56.87%\n",
      "Epoch 760 | Loss: 944079.7500 | RMSE: 973.17 | R¬≤: 0.5721 | MAPE: 57.29%\n",
      "Epoch 770 | Loss: 943827.1875 | RMSE: 972.98 | R¬≤: 0.5722 | MAPE: 56.49%\n",
      "Epoch 780 | Loss: 943598.0000 | RMSE: 973.08 | R¬≤: 0.5722 | MAPE: 56.84%\n",
      "Epoch 790 | Loss: 943355.5625 | RMSE: 972.75 | R¬≤: 0.5724 | MAPE: 56.93%\n",
      "Epoch 800 | Loss: 943115.0625 | RMSE: 972.68 | R¬≤: 0.5725 | MAPE: 56.80%\n",
      "Epoch 810 | Loss: 942867.8125 | RMSE: 972.56 | R¬≤: 0.5726 | MAPE: 56.74%\n",
      "Epoch 820 | Loss: 942604.3125 | RMSE: 972.39 | R¬≤: 0.5728 | MAPE: 56.71%\n",
      "Epoch 830 | Loss: 942438.6250 | RMSE: 972.46 | R¬≤: 0.5727 | MAPE: 56.61%\n",
      "Epoch 840 | Loss: 942071.0000 | RMSE: 971.91 | R¬≤: 0.5732 | MAPE: 56.92%\n",
      "Epoch 850 | Loss: 941692.0000 | RMSE: 971.78 | R¬≤: 0.5733 | MAPE: 56.55%\n",
      "Epoch 860 | Loss: 941245.6250 | RMSE: 971.43 | R¬≤: 0.5736 | MAPE: 56.83%\n",
      "Epoch 870 | Loss: 940802.1250 | RMSE: 971.22 | R¬≤: 0.5738 | MAPE: 56.88%\n",
      "Epoch 880 | Loss: 940304.1250 | RMSE: 970.91 | R¬≤: 0.5741 | MAPE: 56.27%\n",
      "Epoch 890 | Loss: 939646.2500 | RMSE: 970.57 | R¬≤: 0.5744 | MAPE: 56.75%\n",
      "Epoch 900 | Loss: 938959.4375 | RMSE: 970.03 | R¬≤: 0.5748 | MAPE: 56.65%\n",
      "Epoch 910 | Loss: 938211.5000 | RMSE: 969.57 | R¬≤: 0.5752 | MAPE: 56.66%\n",
      "Epoch 920 | Loss: 937319.1250 | RMSE: 969.05 | R¬≤: 0.5757 | MAPE: 56.65%\n",
      "Epoch 930 | Loss: 936416.2500 | RMSE: 968.44 | R¬≤: 0.5762 | MAPE: 56.71%\n",
      "Epoch 940 | Loss: 935444.8750 | RMSE: 967.82 | R¬≤: 0.5768 | MAPE: 56.34%\n",
      "Epoch 950 | Loss: 934297.5625 | RMSE: 966.96 | R¬≤: 0.5775 | MAPE: 56.39%\n",
      "Epoch 960 | Loss: 933014.2500 | RMSE: 966.10 | R¬≤: 0.5783 | MAPE: 56.34%\n",
      "Epoch 970 | Loss: 931671.9375 | RMSE: 965.14 | R¬≤: 0.5791 | MAPE: 56.39%\n",
      "Epoch 980 | Loss: 930220.6250 | RMSE: 964.17 | R¬≤: 0.5800 | MAPE: 56.09%\n",
      "Epoch 990 | Loss: 929799.7500 | RMSE: 963.35 | R¬≤: 0.5807 | MAPE: 55.81%\n",
      "Epoch 1000 | Loss: 927903.5000 | RMSE: 962.90 | R¬≤: 0.5811 | MAPE: 55.78%\n",
      "Epoch 1010 | Loss: 926795.6250 | RMSE: 961.59 | R¬≤: 0.5822 | MAPE: 55.90%\n",
      "Epoch 1020 | Loss: 925641.5625 | RMSE: 960.59 | R¬≤: 0.5831 | MAPE: 55.97%\n",
      "Epoch 1030 | Loss: 924656.6250 | RMSE: 960.09 | R¬≤: 0.5835 | MAPE: 56.20%\n",
      "Epoch 1040 | Loss: 923649.0000 | RMSE: 959.58 | R¬≤: 0.5839 | MAPE: 56.23%\n",
      "Epoch 1050 | Loss: 922667.8125 | RMSE: 959.01 | R¬≤: 0.5844 | MAPE: 55.96%\n",
      "Epoch 1060 | Loss: 975790.7500 | RMSE: 1293.54 | R¬≤: 0.2440 | MAPE: 54.96%\n",
      "Epoch 1070 | Loss: 1815440.2500 | RMSE: 1310.13 | R¬≤: 0.2244 | MAPE: 79.97%\n",
      "Epoch 1080 | Loss: 1317407.8750 | RMSE: 1151.14 | R¬≤: 0.4012 | MAPE: 133.41%\n",
      "Epoch 1090 | Loss: 1178970.1250 | RMSE: 1108.80 | R¬≤: 0.4445 | MAPE: 92.93%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1100 | Loss: 1134294.5000 | RMSE: 1075.46 | R¬≤: 0.4774 | MAPE: 107.89%\n",
      "Epoch 1110 | Loss: 1119472.2500 | RMSE: 1062.87 | R¬≤: 0.4896 | MAPE: 92.45%\n",
      "Epoch 1120 | Loss: 1101194.1250 | RMSE: 1055.02 | R¬≤: 0.4971 | MAPE: 88.10%\n",
      "Epoch 1130 | Loss: 1088221.6250 | RMSE: 1046.89 | R¬≤: 0.5048 | MAPE: 86.34%\n",
      "Epoch 1140 | Loss: 1079073.6250 | RMSE: 1040.87 | R¬≤: 0.5105 | MAPE: 83.43%\n",
      "Epoch 1150 | Loss: 1072185.2500 | RMSE: 1036.52 | R¬≤: 0.5145 | MAPE: 79.59%\n",
      "Epoch 1160 | Loss: 1063930.0000 | RMSE: 1031.70 | R¬≤: 0.5191 | MAPE: 76.19%\n",
      "Epoch 1170 | Loss: 1041651.0000 | RMSE: 1019.47 | R¬≤: 0.5304 | MAPE: 69.80%\n",
      "Epoch 1180 | Loss: 1018089.9375 | RMSE: 1006.80 | R¬≤: 0.5420 | MAPE: 60.57%\n",
      "Epoch 1190 | Loss: 985911.9375 | RMSE: 990.18 | R¬≤: 0.5570 | MAPE: 58.30%\n",
      "Epoch 1200 | Loss: 964674.1875 | RMSE: 981.69 | R¬≤: 0.5645 | MAPE: 57.67%\n",
      "Epoch 1210 | Loss: 953130.5625 | RMSE: 976.97 | R¬≤: 0.5687 | MAPE: 57.95%\n",
      "Epoch 1220 | Loss: 947768.2500 | RMSE: 975.27 | R¬≤: 0.5702 | MAPE: 55.10%\n",
      "Epoch 1230 | Loss: 945887.1250 | RMSE: 973.12 | R¬≤: 0.5721 | MAPE: 57.08%\n",
      "Epoch 1240 | Loss: 942075.6875 | RMSE: 971.61 | R¬≤: 0.5734 | MAPE: 57.76%\n",
      "Epoch 1250 | Loss: 938388.5625 | RMSE: 969.75 | R¬≤: 0.5751 | MAPE: 56.75%\n",
      "Epoch 1260 | Loss: 935283.1250 | RMSE: 967.06 | R¬≤: 0.5774 | MAPE: 56.94%\n",
      "Epoch 1270 | Loss: 932320.3750 | RMSE: 964.89 | R¬≤: 0.5793 | MAPE: 56.87%\n",
      "Epoch 1280 | Loss: 929180.0625 | RMSE: 962.87 | R¬≤: 0.5811 | MAPE: 55.86%\n",
      "Epoch 1290 | Loss: 926873.5625 | RMSE: 961.11 | R¬≤: 0.5826 | MAPE: 56.28%\n",
      "Epoch 1300 | Loss: 925295.1250 | RMSE: 959.72 | R¬≤: 0.5838 | MAPE: 56.74%\n",
      "Epoch 1310 | Loss: 923432.7500 | RMSE: 958.52 | R¬≤: 0.5849 | MAPE: 55.93%\n",
      "Epoch 1320 | Loss: 922129.5625 | RMSE: 957.64 | R¬≤: 0.5856 | MAPE: 55.96%\n",
      "Epoch 1330 | Loss: 921631.6250 | RMSE: 957.02 | R¬≤: 0.5862 | MAPE: 56.27%\n",
      "Epoch 1340 | Loss: 920304.9375 | RMSE: 957.41 | R¬≤: 0.5858 | MAPE: 55.87%\n",
      "Epoch 1350 | Loss: 919367.8125 | RMSE: 956.16 | R¬≤: 0.5869 | MAPE: 56.01%\n",
      "Epoch 1360 | Loss: 918445.5000 | RMSE: 955.87 | R¬≤: 0.5872 | MAPE: 56.02%\n",
      "Epoch 1370 | Loss: 917716.7500 | RMSE: 955.45 | R¬≤: 0.5875 | MAPE: 55.58%\n",
      "Epoch 1380 | Loss: 917033.1250 | RMSE: 954.91 | R¬≤: 0.5880 | MAPE: 55.16%\n",
      "Epoch 1390 | Loss: 916779.6250 | RMSE: 954.43 | R¬≤: 0.5884 | MAPE: 55.38%\n",
      "Epoch 1400 | Loss: 915414.2500 | RMSE: 953.99 | R¬≤: 0.5888 | MAPE: 55.80%\n",
      "Epoch 1410 | Loss: 914707.1250 | RMSE: 954.39 | R¬≤: 0.5884 | MAPE: 56.18%\n",
      "Epoch 1420 | Loss: 913961.1875 | RMSE: 953.32 | R¬≤: 0.5894 | MAPE: 55.16%\n",
      "Epoch 1430 | Loss: 913103.1250 | RMSE: 953.13 | R¬≤: 0.5895 | MAPE: 55.39%\n",
      "Epoch 1440 | Loss: 912445.8125 | RMSE: 952.49 | R¬≤: 0.5901 | MAPE: 55.20%\n",
      "Epoch 1450 | Loss: 911553.7500 | RMSE: 952.31 | R¬≤: 0.5902 | MAPE: 55.42%\n",
      "Epoch 1460 | Loss: 911023.1875 | RMSE: 952.11 | R¬≤: 0.5904 | MAPE: 55.03%\n",
      "Epoch 1470 | Loss: 910800.6875 | RMSE: 952.42 | R¬≤: 0.5901 | MAPE: 56.61%\n",
      "Epoch 1480 | Loss: 909958.8750 | RMSE: 952.19 | R¬≤: 0.5903 | MAPE: 54.82%\n",
      "Epoch 1490 | Loss: 908560.4375 | RMSE: 951.46 | R¬≤: 0.5910 | MAPE: 54.91%\n",
      "Epoch 1500 | Loss: 907802.0000 | RMSE: 951.28 | R¬≤: 0.5911 | MAPE: 55.30%\n",
      "Epoch 1510 | Loss: 906561.3125 | RMSE: 950.11 | R¬≤: 0.5921 | MAPE: 55.76%\n",
      "Epoch 1520 | Loss: 905585.3750 | RMSE: 949.86 | R¬≤: 0.5923 | MAPE: 55.11%\n",
      "Epoch 1530 | Loss: 904333.3750 | RMSE: 949.95 | R¬≤: 0.5922 | MAPE: 54.51%\n",
      "Epoch 1540 | Loss: 903517.4375 | RMSE: 948.93 | R¬≤: 0.5931 | MAPE: 54.96%\n",
      "Epoch 1550 | Loss: 903080.0625 | RMSE: 948.73 | R¬≤: 0.5933 | MAPE: 56.17%\n",
      "Epoch 1560 | Loss: 901470.4375 | RMSE: 947.93 | R¬≤: 0.5940 | MAPE: 55.18%\n",
      "Epoch 1570 | Loss: 900632.6250 | RMSE: 947.58 | R¬≤: 0.5943 | MAPE: 55.42%\n",
      "Epoch 1580 | Loss: 899187.2500 | RMSE: 946.89 | R¬≤: 0.5949 | MAPE: 55.33%\n",
      "Epoch 1590 | Loss: 900827.5625 | RMSE: 946.67 | R¬≤: 0.5951 | MAPE: 55.86%\n",
      "Epoch 1600 | Loss: 899076.1250 | RMSE: 946.61 | R¬≤: 0.5951 | MAPE: 55.35%\n",
      "Epoch 1610 | Loss: 897266.2500 | RMSE: 945.86 | R¬≤: 0.5958 | MAPE: 55.28%\n",
      "Epoch 1620 | Loss: 895899.7500 | RMSE: 945.97 | R¬≤: 0.5957 | MAPE: 55.08%\n",
      "Epoch 1630 | Loss: 895070.0625 | RMSE: 945.53 | R¬≤: 0.5960 | MAPE: 55.82%\n",
      "Epoch 1640 | Loss: 894930.0625 | RMSE: 945.62 | R¬≤: 0.5960 | MAPE: 56.80%\n",
      "Epoch 1650 | Loss: 897213.5000 | RMSE: 945.19 | R¬≤: 0.5963 | MAPE: 55.98%\n",
      "Epoch 1660 | Loss: 895986.5000 | RMSE: 947.47 | R¬≤: 0.5944 | MAPE: 57.34%\n",
      "Epoch 1670 | Loss: 895148.3750 | RMSE: 947.45 | R¬≤: 0.5944 | MAPE: 55.08%\n",
      "Epoch 1680 | Loss: 893300.4375 | RMSE: 944.12 | R¬≤: 0.5972 | MAPE: 56.64%\n",
      "Epoch 1690 | Loss: 892020.7500 | RMSE: 944.00 | R¬≤: 0.5973 | MAPE: 55.60%\n",
      "Epoch 1700 | Loss: 890321.2500 | RMSE: 943.48 | R¬≤: 0.5978 | MAPE: 56.38%\n",
      "Epoch 1710 | Loss: 889704.9375 | RMSE: 942.99 | R¬≤: 0.5982 | MAPE: 55.71%\n",
      "Epoch 1720 | Loss: 889505.1875 | RMSE: 944.49 | R¬≤: 0.5969 | MAPE: 58.38%\n",
      "Epoch 1730 | Loss: 888917.9375 | RMSE: 943.10 | R¬≤: 0.5981 | MAPE: 58.40%\n",
      "Epoch 1740 | Loss: 888376.7500 | RMSE: 942.69 | R¬≤: 0.5985 | MAPE: 55.28%\n",
      "Epoch 1750 | Loss: 887829.1250 | RMSE: 944.19 | R¬≤: 0.5972 | MAPE: 56.73%\n",
      "Epoch 1760 | Loss: 1305996.8750 | RMSE: 1547.25 | R¬≤: -0.0817 | MAPE: 56.34%\n",
      "Epoch 1770 | Loss: 1567347.3750 | RMSE: 1143.68 | R¬≤: 0.4090 | MAPE: 100.86%\n",
      "Epoch 1780 | Loss: 1195583.8750 | RMSE: 1091.99 | R¬≤: 0.4612 | MAPE: 110.53%\n",
      "Epoch 1790 | Loss: 1067039.3750 | RMSE: 1038.32 | R¬≤: 0.5129 | MAPE: 78.96%\n",
      "Epoch 1800 | Loss: 1017653.6875 | RMSE: 1011.90 | R¬≤: 0.5373 | MAPE: 68.12%\n",
      "Epoch 1810 | Loss: 1047424.5625 | RMSE: 1043.74 | R¬≤: 0.5078 | MAPE: 91.00%\n",
      "Epoch 1820 | Loss: 1110443.6250 | RMSE: 1008.16 | R¬≤: 0.5408 | MAPE: 54.29%\n",
      "Epoch 1830 | Loss: 969396.3125 | RMSE: 1026.44 | R¬≤: 0.5239 | MAPE: 51.31%\n",
      "Epoch 1840 | Loss: 1055480.3750 | RMSE: 998.45 | R¬≤: 0.5496 | MAPE: 54.69%\n",
      "Epoch 1850 | Loss: 966424.9375 | RMSE: 989.63 | R¬≤: 0.5575 | MAPE: 54.38%\n",
      "Epoch 1860 | Loss: 956913.6250 | RMSE: 983.23 | R¬≤: 0.5632 | MAPE: 54.41%\n",
      "Epoch 1870 | Loss: 950485.3125 | RMSE: 974.74 | R¬≤: 0.5707 | MAPE: 56.77%\n",
      "Epoch 1880 | Loss: 944628.9375 | RMSE: 973.11 | R¬≤: 0.5721 | MAPE: 57.43%\n",
      "Epoch 1890 | Loss: 942592.2500 | RMSE: 971.41 | R¬≤: 0.5736 | MAPE: 56.29%\n",
      "Epoch 1900 | Loss: 940022.5000 | RMSE: 970.32 | R¬≤: 0.5746 | MAPE: 55.49%\n",
      "Epoch 1910 | Loss: 938010.3750 | RMSE: 969.20 | R¬≤: 0.5756 | MAPE: 56.22%\n",
      "Epoch 1920 | Loss: 936079.8125 | RMSE: 968.14 | R¬≤: 0.5765 | MAPE: 56.04%\n",
      "Epoch 1930 | Loss: 934135.4375 | RMSE: 967.08 | R¬≤: 0.5774 | MAPE: 55.77%\n",
      "Epoch 1940 | Loss: 931993.5625 | RMSE: 965.96 | R¬≤: 0.5784 | MAPE: 55.59%\n",
      "Epoch 1950 | Loss: 929097.8750 | RMSE: 964.45 | R¬≤: 0.5797 | MAPE: 55.29%\n",
      "Epoch 1960 | Loss: 926501.1250 | RMSE: 962.49 | R¬≤: 0.5814 | MAPE: 55.92%\n",
      "Epoch 1970 | Loss: 923237.8750 | RMSE: 960.68 | R¬≤: 0.5830 | MAPE: 55.97%\n",
      "Epoch 1980 | Loss: 919797.7500 | RMSE: 958.86 | R¬≤: 0.5846 | MAPE: 55.05%\n",
      "Epoch 1990 | Loss: 916154.3125 | RMSE: 956.21 | R¬≤: 0.5869 | MAPE: 55.32%\n",
      "‚úÖ Submission saved to: submission.csv\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_predict_final(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    test_final=test[predictors],\n",
    "    test_ids=test[['Item_Identifier', 'Outlet_Identifier']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ce3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf091411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5d045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "770f0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# ---------------------- MLP Definition ----------------------\n",
    "import torch.nn as nn\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # Dropout added here (deeper stage)\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "    \n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(dropout_rate), \n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Training + Prediction ----------------------\n",
    "def evaluate_and_predict_final(X_train, X_test, y_train, y_test, test_final, test_ids, submission_file=\"submission.csv\", model_path=\"best_model.pt\"):\n",
    "    \"\"\"\n",
    "    Trains PyTorch MLP on X_train/y_train, evaluates on X_test/y_test.\n",
    "    Saves best model (lowest RMSE) to model_path.\n",
    "    Predicts on test_final and saves submission to CSV with test_ids.\n",
    "    \"\"\"\n",
    "    # Ensure numpy\n",
    "    X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    X_test = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "    y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "    y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "    test_final = test_final.values if isinstance(test_final, pd.DataFrame) else test_final\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    model = TorchMLP(X_train.shape[1]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    print(\"üî• Training PyTorch MLP...\")\n",
    "    for epoch in range(1, 2000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_test_tensor).cpu().numpy().flatten()\n",
    "                val_true = y_test_tensor.cpu().numpy().flatten()\n",
    "                rmse = mean_squared_error(val_true, val_preds, squared=False)\n",
    "                r2 = r2_score(val_true, val_preds)\n",
    "                mape = mean_absolute_percentage_error(val_true, val_preds) * 100\n",
    "\n",
    "                print(f\"Epoch {epoch:04d} | Loss: {loss.item():.4f} | RMSE: {rmse:.2f} | R¬≤: {r2:.4f} | MAPE: {mape:.2f}%\")\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print(f\"üíæ Best model saved at epoch {epoch} (RMSE: {rmse:.2f})\")\n",
    "\n",
    "    # üîÅ Reload best model before final prediction\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    X_final_tensor = torch.tensor(test_final, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        final_preds = model(X_final_tensor).cpu().numpy().flatten()\n",
    "        final_preds = np.clip(final_preds, 0, None)\n",
    "\n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        'Item_Identifier': test_ids['Item_Identifier'].values,\n",
    "        'Outlet_Identifier': test_ids['Outlet_Identifier'].values,\n",
    "        'Item_Outlet_Sales': final_preds\n",
    "    })\n",
    "\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"‚úÖ Submission saved to: {submission_file}\")\n",
    "    print(f\"üì¶ Best model weights saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a7045413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9a8cf0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "22a99999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "915286ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Training PyTorch MLP...\n",
      "Epoch 0001 | Loss: 7842857.0000 | RMSE: 2673.48 | R¬≤: -1.5906 | MAPE: 99.96%\n",
      "üíæ Best model saved at epoch 1 (RMSE: 2673.48)\n",
      "Epoch 0010 | Loss: 4742855.0000 | RMSE: 2086.57 | R¬≤: -0.5780 | MAPE: 270.21%\n",
      "üíæ Best model saved at epoch 10 (RMSE: 2086.57)\n",
      "Epoch 0020 | Loss: 3285808.5000 | RMSE: 2036.82 | R¬≤: -0.5037 | MAPE: 171.10%\n",
      "üíæ Best model saved at epoch 20 (RMSE: 2036.82)\n",
      "Epoch 0030 | Loss: 3019185.2500 | RMSE: 1675.83 | R¬≤: -0.0179 | MAPE: 152.78%\n",
      "üíæ Best model saved at epoch 30 (RMSE: 1675.83)\n",
      "Epoch 0040 | Loss: 2758077.5000 | RMSE: 1607.87 | R¬≤: 0.0630 | MAPE: 173.83%\n",
      "üíæ Best model saved at epoch 40 (RMSE: 1607.87)\n",
      "Epoch 0050 | Loss: 2426232.2500 | RMSE: 1501.09 | R¬≤: 0.1833 | MAPE: 137.39%\n",
      "üíæ Best model saved at epoch 50 (RMSE: 1501.09)\n",
      "Epoch 0060 | Loss: 2203018.2500 | RMSE: 1491.75 | R¬≤: 0.1934 | MAPE: 113.40%\n",
      "üíæ Best model saved at epoch 60 (RMSE: 1491.75)\n",
      "Epoch 0070 | Loss: 2031223.5000 | RMSE: 1373.33 | R¬≤: 0.3164 | MAPE: 106.59%\n",
      "üíæ Best model saved at epoch 70 (RMSE: 1373.33)\n",
      "Epoch 0080 | Loss: 1828681.7500 | RMSE: 1300.37 | R¬≤: 0.3871 | MAPE: 98.43%\n",
      "üíæ Best model saved at epoch 80 (RMSE: 1300.37)\n",
      "Epoch 0090 | Loss: 1623124.2500 | RMSE: 1229.84 | R¬≤: 0.4518 | MAPE: 83.13%\n",
      "üíæ Best model saved at epoch 90 (RMSE: 1229.84)\n",
      "Epoch 0100 | Loss: 1406878.2500 | RMSE: 1157.37 | R¬≤: 0.5145 | MAPE: 72.93%\n",
      "üíæ Best model saved at epoch 100 (RMSE: 1157.37)\n",
      "Epoch 0110 | Loss: 1299499.8750 | RMSE: 1118.54 | R¬≤: 0.5465 | MAPE: 62.16%\n",
      "üíæ Best model saved at epoch 110 (RMSE: 1118.54)\n",
      "Epoch 0120 | Loss: 1283676.0000 | RMSE: 1106.84 | R¬≤: 0.5560 | MAPE: 60.08%\n",
      "üíæ Best model saved at epoch 120 (RMSE: 1106.84)\n",
      "Epoch 0130 | Loss: 1253593.0000 | RMSE: 1090.78 | R¬≤: 0.5688 | MAPE: 61.42%\n",
      "üíæ Best model saved at epoch 130 (RMSE: 1090.78)\n",
      "Epoch 0140 | Loss: 1240926.7500 | RMSE: 1069.09 | R¬≤: 0.5857 | MAPE: 57.94%\n",
      "üíæ Best model saved at epoch 140 (RMSE: 1069.09)\n",
      "Epoch 0150 | Loss: 1226118.3750 | RMSE: 1057.32 | R¬≤: 0.5948 | MAPE: 54.09%\n",
      "üíæ Best model saved at epoch 150 (RMSE: 1057.32)\n",
      "Epoch 0160 | Loss: 1226815.8750 | RMSE: 1082.21 | R¬≤: 0.5755 | MAPE: 50.28%\n",
      "Epoch 0170 | Loss: 1230671.6250 | RMSE: 1043.66 | R¬≤: 0.6052 | MAPE: 54.35%\n",
      "üíæ Best model saved at epoch 170 (RMSE: 1043.66)\n",
      "Epoch 0180 | Loss: 1210871.8750 | RMSE: 1050.65 | R¬≤: 0.5999 | MAPE: 51.48%\n",
      "Epoch 0190 | Loss: 1204354.3750 | RMSE: 1041.48 | R¬≤: 0.6069 | MAPE: 53.95%\n",
      "üíæ Best model saved at epoch 190 (RMSE: 1041.48)\n",
      "Epoch 0200 | Loss: 1201022.3750 | RMSE: 1039.39 | R¬≤: 0.6084 | MAPE: 53.39%\n",
      "üíæ Best model saved at epoch 200 (RMSE: 1039.39)\n",
      "Epoch 0210 | Loss: 1219260.1250 | RMSE: 1056.26 | R¬≤: 0.5956 | MAPE: 50.06%\n",
      "Epoch 0220 | Loss: 1193786.6250 | RMSE: 1042.14 | R¬≤: 0.6064 | MAPE: 52.41%\n",
      "Epoch 0230 | Loss: 1221420.2500 | RMSE: 1041.34 | R¬≤: 0.6070 | MAPE: 56.75%\n",
      "Epoch 0240 | Loss: 1189435.8750 | RMSE: 1053.01 | R¬≤: 0.5981 | MAPE: 51.12%\n",
      "Epoch 0250 | Loss: 1198404.6250 | RMSE: 1050.41 | R¬≤: 0.6001 | MAPE: 52.80%\n",
      "Epoch 0260 | Loss: 1187162.8750 | RMSE: 1044.93 | R¬≤: 0.6042 | MAPE: 51.51%\n",
      "Epoch 0270 | Loss: 1187982.6250 | RMSE: 1046.51 | R¬≤: 0.6030 | MAPE: 51.19%\n",
      "Epoch 0280 | Loss: 1201345.3750 | RMSE: 1045.94 | R¬≤: 0.6035 | MAPE: 58.54%\n",
      "Epoch 0290 | Loss: 1199452.2500 | RMSE: 1035.40 | R¬≤: 0.6114 | MAPE: 57.00%\n",
      "üíæ Best model saved at epoch 290 (RMSE: 1035.40)\n",
      "Epoch 0300 | Loss: 1194492.3750 | RMSE: 1055.77 | R¬≤: 0.5960 | MAPE: 50.45%\n",
      "Epoch 0310 | Loss: 1192131.7500 | RMSE: 1053.99 | R¬≤: 0.5974 | MAPE: 50.08%\n",
      "Epoch 0320 | Loss: 1180287.0000 | RMSE: 1044.44 | R¬≤: 0.6046 | MAPE: 51.86%\n",
      "Epoch 0330 | Loss: 1186291.3750 | RMSE: 1039.20 | R¬≤: 0.6086 | MAPE: 53.54%\n",
      "Epoch 0340 | Loss: 1176889.6250 | RMSE: 1044.69 | R¬≤: 0.6044 | MAPE: 51.78%\n",
      "Epoch 0350 | Loss: 1214202.6250 | RMSE: 1080.59 | R¬≤: 0.5768 | MAPE: 48.49%\n",
      "Epoch 0360 | Loss: 1182841.8750 | RMSE: 1038.19 | R¬≤: 0.6093 | MAPE: 58.16%\n",
      "Epoch 0370 | Loss: 1187184.6250 | RMSE: 1062.91 | R¬≤: 0.5905 | MAPE: 50.03%\n",
      "Epoch 0380 | Loss: 1191524.0000 | RMSE: 1047.80 | R¬≤: 0.6021 | MAPE: 51.04%\n",
      "Epoch 0390 | Loss: 1175595.7500 | RMSE: 1042.57 | R¬≤: 0.6060 | MAPE: 51.69%\n",
      "Epoch 0400 | Loss: 1184507.1250 | RMSE: 1058.08 | R¬≤: 0.5942 | MAPE: 49.75%\n",
      "Epoch 0410 | Loss: 1179291.3750 | RMSE: 1046.39 | R¬≤: 0.6031 | MAPE: 51.65%\n",
      "Epoch 0420 | Loss: 1170666.7500 | RMSE: 1048.12 | R¬≤: 0.6018 | MAPE: 51.38%\n",
      "Epoch 0430 | Loss: 1172146.7500 | RMSE: 1058.80 | R¬≤: 0.5937 | MAPE: 50.15%\n",
      "Epoch 0440 | Loss: 1223334.6250 | RMSE: 1039.02 | R¬≤: 0.6087 | MAPE: 58.09%\n",
      "Epoch 0450 | Loss: 1183935.0000 | RMSE: 1040.55 | R¬≤: 0.6076 | MAPE: 54.55%\n",
      "Epoch 0460 | Loss: 1180156.0000 | RMSE: 1058.93 | R¬≤: 0.5936 | MAPE: 49.56%\n",
      "Epoch 0470 | Loss: 1180928.6250 | RMSE: 1060.03 | R¬≤: 0.5927 | MAPE: 49.89%\n",
      "Epoch 0480 | Loss: 1188315.3750 | RMSE: 1058.97 | R¬≤: 0.5935 | MAPE: 50.69%\n",
      "Epoch 0490 | Loss: 1172579.3750 | RMSE: 1085.04 | R¬≤: 0.5733 | MAPE: 47.99%\n",
      "Epoch 0500 | Loss: 1168457.5000 | RMSE: 1058.88 | R¬≤: 0.5936 | MAPE: 50.17%\n",
      "Epoch 0510 | Loss: 1172653.8750 | RMSE: 1066.81 | R¬≤: 0.5875 | MAPE: 49.13%\n",
      "Epoch 0520 | Loss: 1202799.6250 | RMSE: 1040.86 | R¬≤: 0.6073 | MAPE: 55.08%\n",
      "Epoch 0530 | Loss: 1180961.0000 | RMSE: 1058.42 | R¬≤: 0.5940 | MAPE: 50.09%\n",
      "Epoch 0540 | Loss: 1182257.6250 | RMSE: 1049.34 | R¬≤: 0.6009 | MAPE: 52.26%\n",
      "Epoch 0550 | Loss: 1183578.8750 | RMSE: 1121.10 | R¬≤: 0.5444 | MAPE: 46.82%\n",
      "Epoch 0560 | Loss: 1176816.0000 | RMSE: 1061.16 | R¬≤: 0.5919 | MAPE: 50.08%\n",
      "Epoch 0570 | Loss: 1175595.6250 | RMSE: 1069.19 | R¬≤: 0.5857 | MAPE: 49.64%\n",
      "Epoch 0580 | Loss: 1171028.5000 | RMSE: 1063.04 | R¬≤: 0.5904 | MAPE: 50.09%\n",
      "Epoch 0590 | Loss: 1167481.7500 | RMSE: 1078.84 | R¬≤: 0.5781 | MAPE: 48.69%\n",
      "Epoch 0600 | Loss: 1181389.1250 | RMSE: 1051.02 | R¬≤: 0.5996 | MAPE: 52.17%\n",
      "Epoch 0610 | Loss: 1172247.0000 | RMSE: 1056.97 | R¬≤: 0.5951 | MAPE: 50.96%\n",
      "Epoch 0620 | Loss: 1184483.1250 | RMSE: 1059.13 | R¬≤: 0.5934 | MAPE: 50.54%\n",
      "Epoch 0630 | Loss: 1178678.2500 | RMSE: 1066.04 | R¬≤: 0.5881 | MAPE: 50.59%\n",
      "Epoch 0640 | Loss: 1159492.7500 | RMSE: 1067.43 | R¬≤: 0.5870 | MAPE: 50.10%\n",
      "Epoch 0650 | Loss: 1171602.2500 | RMSE: 1076.63 | R¬≤: 0.5799 | MAPE: 49.31%\n",
      "Epoch 0660 | Loss: 1180558.8750 | RMSE: 1088.34 | R¬≤: 0.5707 | MAPE: 48.20%\n",
      "Epoch 0670 | Loss: 1167902.7500 | RMSE: 1112.20 | R¬≤: 0.5517 | MAPE: 47.65%\n",
      "Epoch 0680 | Loss: 1168550.1250 | RMSE: 1143.83 | R¬≤: 0.5258 | MAPE: 46.30%\n",
      "Epoch 0690 | Loss: 1167814.8750 | RMSE: 1111.46 | R¬≤: 0.5522 | MAPE: 47.43%\n",
      "Epoch 0700 | Loss: 1162885.7500 | RMSE: 1115.41 | R¬≤: 0.5491 | MAPE: 47.53%\n",
      "Epoch 0710 | Loss: 1159157.1250 | RMSE: 1100.62 | R¬≤: 0.5609 | MAPE: 47.82%\n",
      "Epoch 0720 | Loss: 1170025.6250 | RMSE: 1153.89 | R¬≤: 0.5174 | MAPE: 46.16%\n",
      "Epoch 0730 | Loss: 1186282.8750 | RMSE: 1085.78 | R¬≤: 0.5727 | MAPE: 49.56%\n",
      "Epoch 0740 | Loss: 1175750.0000 | RMSE: 1156.61 | R¬≤: 0.5151 | MAPE: 46.05%\n",
      "Epoch 0750 | Loss: 1173097.6250 | RMSE: 1103.88 | R¬≤: 0.5583 | MAPE: 48.43%\n",
      "Epoch 0760 | Loss: 1168485.6250 | RMSE: 1141.45 | R¬≤: 0.5278 | MAPE: 46.60%\n",
      "Epoch 0770 | Loss: 1162639.5000 | RMSE: 1171.37 | R¬≤: 0.5027 | MAPE: 45.99%\n",
      "Epoch 0780 | Loss: 1230496.0000 | RMSE: 1330.35 | R¬≤: 0.3585 | MAPE: 46.37%\n",
      "Epoch 0790 | Loss: 1186512.7500 | RMSE: 1189.28 | R¬≤: 0.4874 | MAPE: 45.47%\n",
      "Epoch 0800 | Loss: 1160574.1250 | RMSE: 1116.45 | R¬≤: 0.5482 | MAPE: 47.48%\n",
      "Epoch 0810 | Loss: 1163808.7500 | RMSE: 1127.07 | R¬≤: 0.5396 | MAPE: 47.43%\n",
      "Epoch 0820 | Loss: 1163803.8750 | RMSE: 1184.09 | R¬≤: 0.4918 | MAPE: 45.74%\n",
      "Epoch 0830 | Loss: 1157765.8750 | RMSE: 1196.66 | R¬≤: 0.4810 | MAPE: 45.66%\n",
      "Epoch 0840 | Loss: 1163111.2500 | RMSE: 1177.22 | R¬≤: 0.4977 | MAPE: 45.92%\n",
      "Epoch 0850 | Loss: 1161165.1250 | RMSE: 1154.44 | R¬≤: 0.5169 | MAPE: 46.54%\n",
      "Epoch 0860 | Loss: 1164556.7500 | RMSE: 1194.64 | R¬≤: 0.4827 | MAPE: 45.75%\n",
      "Epoch 0870 | Loss: 1166243.5000 | RMSE: 1180.60 | R¬≤: 0.4948 | MAPE: 45.87%\n",
      "Epoch 0880 | Loss: 1156073.8750 | RMSE: 1169.91 | R¬≤: 0.5039 | MAPE: 46.08%\n",
      "Epoch 0890 | Loss: 1171110.8750 | RMSE: 1212.64 | R¬≤: 0.4670 | MAPE: 45.53%\n",
      "Epoch 0900 | Loss: 1162908.8750 | RMSE: 1155.69 | R¬≤: 0.5159 | MAPE: 46.36%\n",
      "Epoch 0910 | Loss: 1164033.7500 | RMSE: 1177.62 | R¬≤: 0.4974 | MAPE: 45.83%\n",
      "Epoch 0920 | Loss: 1161560.0000 | RMSE: 1170.54 | R¬≤: 0.5034 | MAPE: 46.04%\n",
      "Epoch 0930 | Loss: 1172013.6250 | RMSE: 1165.90 | R¬≤: 0.5073 | MAPE: 46.15%\n",
      "Epoch 0940 | Loss: 1163751.3750 | RMSE: 1188.62 | R¬≤: 0.4879 | MAPE: 45.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0950 | Loss: 1176650.7500 | RMSE: 1143.67 | R¬≤: 0.5259 | MAPE: 47.26%\n",
      "Epoch 0960 | Loss: 1181176.7500 | RMSE: 1258.92 | R¬≤: 0.4256 | MAPE: 45.50%\n",
      "Epoch 0970 | Loss: 1170875.3750 | RMSE: 1154.97 | R¬≤: 0.5165 | MAPE: 46.74%\n",
      "Epoch 0980 | Loss: 1164911.5000 | RMSE: 1197.52 | R¬≤: 0.4802 | MAPE: 45.75%\n",
      "Epoch 0990 | Loss: 1160203.2500 | RMSE: 1186.81 | R¬≤: 0.4895 | MAPE: 45.81%\n",
      "Epoch 1000 | Loss: 1164831.6250 | RMSE: 1175.28 | R¬≤: 0.4994 | MAPE: 46.27%\n",
      "Epoch 1010 | Loss: 1170745.0000 | RMSE: 1161.88 | R¬≤: 0.5107 | MAPE: 47.06%\n",
      "Epoch 1020 | Loss: 1169717.2500 | RMSE: 1234.82 | R¬≤: 0.4473 | MAPE: 45.46%\n",
      "Epoch 1030 | Loss: 1163270.5000 | RMSE: 1191.68 | R¬≤: 0.4853 | MAPE: 45.78%\n",
      "Epoch 1040 | Loss: 1164989.0000 | RMSE: 1210.82 | R¬≤: 0.4686 | MAPE: 45.54%\n",
      "Epoch 1050 | Loss: 1165050.1250 | RMSE: 1184.61 | R¬≤: 0.4914 | MAPE: 45.89%\n",
      "Epoch 1060 | Loss: 1170838.3750 | RMSE: 1150.75 | R¬≤: 0.5200 | MAPE: 47.02%\n",
      "Epoch 1070 | Loss: 1158922.5000 | RMSE: 1153.80 | R¬≤: 0.5175 | MAPE: 46.52%\n",
      "Epoch 1080 | Loss: 1155175.8750 | RMSE: 1206.17 | R¬≤: 0.4727 | MAPE: 45.63%\n",
      "Epoch 1090 | Loss: 1163003.8750 | RMSE: 1187.51 | R¬≤: 0.4889 | MAPE: 45.86%\n",
      "Epoch 1100 | Loss: 1163754.1250 | RMSE: 1232.97 | R¬≤: 0.4490 | MAPE: 45.42%\n",
      "Epoch 1110 | Loss: 1180521.2500 | RMSE: 1130.61 | R¬≤: 0.5367 | MAPE: 47.94%\n",
      "Epoch 1120 | Loss: 1157914.0000 | RMSE: 1219.04 | R¬≤: 0.4614 | MAPE: 45.50%\n",
      "Epoch 1130 | Loss: 1166370.7500 | RMSE: 1162.71 | R¬≤: 0.5100 | MAPE: 46.62%\n",
      "Epoch 1140 | Loss: 1161799.1250 | RMSE: 1226.67 | R¬≤: 0.4546 | MAPE: 45.51%\n",
      "Epoch 1150 | Loss: 1164716.7500 | RMSE: 1196.58 | R¬≤: 0.4810 | MAPE: 45.68%\n",
      "Epoch 1160 | Loss: 1168441.3750 | RMSE: 1156.06 | R¬≤: 0.5156 | MAPE: 46.66%\n",
      "Epoch 1170 | Loss: 1158896.3750 | RMSE: 1149.16 | R¬≤: 0.5214 | MAPE: 46.98%\n",
      "Epoch 1180 | Loss: 1166334.1250 | RMSE: 1233.45 | R¬≤: 0.4486 | MAPE: 45.62%\n",
      "Epoch 1190 | Loss: 1159257.8750 | RMSE: 1205.02 | R¬≤: 0.4737 | MAPE: 45.72%\n",
      "Epoch 1200 | Loss: 1158926.6250 | RMSE: 1186.45 | R¬≤: 0.4898 | MAPE: 46.13%\n",
      "Epoch 1210 | Loss: 1157881.2500 | RMSE: 1251.65 | R¬≤: 0.4322 | MAPE: 45.53%\n",
      "Epoch 1220 | Loss: 1158500.3750 | RMSE: 1193.17 | R¬≤: 0.4840 | MAPE: 46.48%\n",
      "Epoch 1230 | Loss: 1155768.7500 | RMSE: 1226.21 | R¬≤: 0.4550 | MAPE: 45.79%\n",
      "Epoch 1240 | Loss: 1158203.5000 | RMSE: 1183.17 | R¬≤: 0.4926 | MAPE: 46.43%\n",
      "Epoch 1250 | Loss: 1159864.1250 | RMSE: 1205.09 | R¬≤: 0.4736 | MAPE: 45.93%\n",
      "Epoch 1260 | Loss: 1171289.7500 | RMSE: 1170.14 | R¬≤: 0.5037 | MAPE: 46.91%\n",
      "Epoch 1270 | Loss: 1165805.0000 | RMSE: 1223.32 | R¬≤: 0.4576 | MAPE: 45.77%\n",
      "Epoch 1280 | Loss: 1159085.8750 | RMSE: 1222.20 | R¬≤: 0.4586 | MAPE: 45.64%\n",
      "Epoch 1290 | Loss: 1154607.7500 | RMSE: 1220.10 | R¬≤: 0.4604 | MAPE: 45.55%\n",
      "Epoch 1300 | Loss: 1165164.7500 | RMSE: 1176.76 | R¬≤: 0.4981 | MAPE: 46.51%\n",
      "Epoch 1310 | Loss: 1158683.7500 | RMSE: 1266.37 | R¬≤: 0.4187 | MAPE: 45.56%\n",
      "Epoch 1320 | Loss: 1160835.2500 | RMSE: 1193.21 | R¬≤: 0.4840 | MAPE: 46.09%\n",
      "Epoch 1330 | Loss: 1157567.1250 | RMSE: 1225.11 | R¬≤: 0.4560 | MAPE: 45.58%\n",
      "Epoch 1340 | Loss: 1155639.8750 | RMSE: 1247.25 | R¬≤: 0.4362 | MAPE: 45.64%\n",
      "Epoch 1350 | Loss: 1201229.2500 | RMSE: 1344.92 | R¬≤: 0.3444 | MAPE: 45.74%\n",
      "Epoch 1360 | Loss: 1159972.1250 | RMSE: 1299.57 | R¬≤: 0.3879 | MAPE: 45.62%\n",
      "Epoch 1370 | Loss: 1151096.7500 | RMSE: 1237.28 | R¬≤: 0.4451 | MAPE: 45.58%\n",
      "Epoch 1380 | Loss: 1155667.1250 | RMSE: 1195.27 | R¬≤: 0.4822 | MAPE: 45.90%\n",
      "Epoch 1390 | Loss: 1152407.1250 | RMSE: 1197.68 | R¬≤: 0.4801 | MAPE: 45.86%\n",
      "Epoch 1400 | Loss: 1155375.2500 | RMSE: 1242.21 | R¬≤: 0.4407 | MAPE: 45.52%\n",
      "Epoch 1410 | Loss: 1154369.7500 | RMSE: 1207.50 | R¬≤: 0.4715 | MAPE: 45.80%\n",
      "Epoch 1420 | Loss: 1155033.2500 | RMSE: 1219.30 | R¬≤: 0.4611 | MAPE: 45.97%\n",
      "Epoch 1430 | Loss: 1153840.5000 | RMSE: 1248.14 | R¬≤: 0.4353 | MAPE: 45.60%\n",
      "Epoch 1440 | Loss: 1160822.1250 | RMSE: 1286.32 | R¬≤: 0.4003 | MAPE: 45.56%\n",
      "Epoch 1450 | Loss: 1153215.2500 | RMSE: 1242.34 | R¬≤: 0.4406 | MAPE: 45.64%\n",
      "Epoch 1460 | Loss: 1155924.6250 | RMSE: 1251.67 | R¬≤: 0.4322 | MAPE: 45.57%\n",
      "Epoch 1470 | Loss: 1154882.6250 | RMSE: 1224.09 | R¬≤: 0.4569 | MAPE: 45.54%\n",
      "Epoch 1480 | Loss: 1154354.7500 | RMSE: 1273.49 | R¬≤: 0.4122 | MAPE: 45.57%\n",
      "Epoch 1490 | Loss: 1153196.8750 | RMSE: 1241.44 | R¬≤: 0.4414 | MAPE: 45.62%\n",
      "Epoch 1500 | Loss: 1165570.2500 | RMSE: 1183.83 | R¬≤: 0.4920 | MAPE: 46.52%\n",
      "Epoch 1510 | Loss: 1153411.1250 | RMSE: 1227.20 | R¬≤: 0.4541 | MAPE: 45.63%\n",
      "Epoch 1520 | Loss: 1159908.0000 | RMSE: 1200.07 | R¬≤: 0.4780 | MAPE: 46.20%\n",
      "Epoch 1530 | Loss: 1154680.6250 | RMSE: 1234.67 | R¬≤: 0.4475 | MAPE: 45.57%\n",
      "Epoch 1540 | Loss: 1153255.6250 | RMSE: 1222.93 | R¬≤: 0.4579 | MAPE: 45.76%\n",
      "Epoch 1550 | Loss: 1187045.7500 | RMSE: 1163.04 | R¬≤: 0.5097 | MAPE: 47.55%\n",
      "Epoch 1560 | Loss: 1170726.6250 | RMSE: 1277.65 | R¬≤: 0.4083 | MAPE: 45.52%\n",
      "Epoch 1570 | Loss: 1158476.6250 | RMSE: 1264.46 | R¬≤: 0.4205 | MAPE: 45.61%\n",
      "Epoch 1580 | Loss: 1150660.3750 | RMSE: 1238.16 | R¬≤: 0.4443 | MAPE: 45.60%\n",
      "Epoch 1590 | Loss: 1158690.7500 | RMSE: 1232.90 | R¬≤: 0.4491 | MAPE: 45.79%\n",
      "Epoch 1600 | Loss: 1151077.0000 | RMSE: 1267.68 | R¬≤: 0.4175 | MAPE: 45.55%\n",
      "Epoch 1610 | Loss: 1156769.2500 | RMSE: 1243.63 | R¬≤: 0.4394 | MAPE: 45.49%\n",
      "Epoch 1620 | Loss: 1157042.0000 | RMSE: 1281.12 | R¬≤: 0.4051 | MAPE: 45.49%\n",
      "Epoch 1630 | Loss: 1165740.7500 | RMSE: 1288.23 | R¬≤: 0.3985 | MAPE: 45.39%\n",
      "Epoch 1640 | Loss: 1160707.2500 | RMSE: 1185.52 | R¬≤: 0.4906 | MAPE: 46.06%\n",
      "Epoch 1650 | Loss: 1166153.0000 | RMSE: 1292.20 | R¬≤: 0.3948 | MAPE: 45.66%\n",
      "Epoch 1660 | Loss: 1164606.0000 | RMSE: 1183.04 | R¬≤: 0.4927 | MAPE: 46.05%\n",
      "Epoch 1670 | Loss: 1153197.0000 | RMSE: 1266.31 | R¬≤: 0.4188 | MAPE: 45.60%\n",
      "Epoch 1680 | Loss: 1154608.5000 | RMSE: 1220.64 | R¬≤: 0.4600 | MAPE: 45.73%\n",
      "Epoch 1690 | Loss: 1154333.8750 | RMSE: 1203.96 | R¬≤: 0.4746 | MAPE: 45.87%\n",
      "Epoch 1700 | Loss: 1192863.3750 | RMSE: 1138.46 | R¬≤: 0.5302 | MAPE: 48.00%\n",
      "Epoch 1710 | Loss: 1163110.7500 | RMSE: 1257.64 | R¬≤: 0.4267 | MAPE: 45.41%\n",
      "Epoch 1720 | Loss: 1156456.0000 | RMSE: 1288.82 | R¬≤: 0.3979 | MAPE: 45.63%\n",
      "Epoch 1730 | Loss: 1151565.3750 | RMSE: 1253.20 | R¬≤: 0.4308 | MAPE: 45.57%\n",
      "Epoch 1740 | Loss: 1151779.8750 | RMSE: 1225.19 | R¬≤: 0.4559 | MAPE: 45.96%\n",
      "Epoch 1750 | Loss: 1157671.1250 | RMSE: 1277.81 | R¬≤: 0.4082 | MAPE: 45.59%\n",
      "Epoch 1760 | Loss: 1155875.8750 | RMSE: 1222.60 | R¬≤: 0.4582 | MAPE: 45.89%\n",
      "Epoch 1770 | Loss: 1153726.5000 | RMSE: 1247.46 | R¬≤: 0.4360 | MAPE: 45.52%\n",
      "Epoch 1780 | Loss: 1164017.5000 | RMSE: 1309.01 | R¬≤: 0.3789 | MAPE: 45.59%\n",
      "Epoch 1790 | Loss: 1156305.2500 | RMSE: 1231.65 | R¬≤: 0.4502 | MAPE: 45.57%\n",
      "Epoch 1800 | Loss: 1156959.5000 | RMSE: 1206.29 | R¬≤: 0.4726 | MAPE: 45.91%\n",
      "Epoch 1810 | Loss: 1157518.2500 | RMSE: 1197.40 | R¬≤: 0.4803 | MAPE: 46.50%\n",
      "Epoch 1820 | Loss: 1152224.6250 | RMSE: 1253.32 | R¬≤: 0.4307 | MAPE: 45.63%\n",
      "Epoch 1830 | Loss: 1158486.7500 | RMSE: 1276.82 | R¬≤: 0.4091 | MAPE: 45.55%\n",
      "Epoch 1840 | Loss: 1156326.0000 | RMSE: 1260.16 | R¬≤: 0.4244 | MAPE: 45.52%\n",
      "Epoch 1850 | Loss: 1151115.8750 | RMSE: 1248.28 | R¬≤: 0.4352 | MAPE: 45.52%\n",
      "Epoch 1860 | Loss: 1186436.8750 | RMSE: 1359.47 | R¬≤: 0.3301 | MAPE: 45.86%\n",
      "Epoch 1870 | Loss: 1160018.8750 | RMSE: 1241.72 | R¬≤: 0.4411 | MAPE: 45.58%\n",
      "Epoch 1880 | Loss: 1165842.3750 | RMSE: 1217.75 | R¬≤: 0.4625 | MAPE: 46.23%\n",
      "Epoch 1890 | Loss: 1150130.6250 | RMSE: 1257.56 | R¬≤: 0.4268 | MAPE: 45.53%\n",
      "Epoch 1900 | Loss: 1155533.2500 | RMSE: 1285.80 | R¬≤: 0.4008 | MAPE: 45.53%\n",
      "Epoch 1910 | Loss: 1150887.2500 | RMSE: 1228.02 | R¬≤: 0.4534 | MAPE: 45.60%\n",
      "Epoch 1920 | Loss: 1151738.3750 | RMSE: 1258.90 | R¬≤: 0.4256 | MAPE: 45.53%\n",
      "Epoch 1930 | Loss: 1147956.7500 | RMSE: 1270.25 | R¬≤: 0.4152 | MAPE: 45.55%\n",
      "Epoch 1940 | Loss: 1160597.5000 | RMSE: 1299.29 | R¬≤: 0.3881 | MAPE: 45.60%\n",
      "Epoch 1950 | Loss: 1151452.5000 | RMSE: 1212.71 | R¬≤: 0.4670 | MAPE: 45.74%\n",
      "Epoch 1960 | Loss: 1152553.7500 | RMSE: 1258.13 | R¬≤: 0.4263 | MAPE: 45.57%\n",
      "Epoch 1970 | Loss: 1172004.5000 | RMSE: 1315.06 | R¬≤: 0.3732 | MAPE: 45.58%\n",
      "Epoch 1980 | Loss: 1160865.5000 | RMSE: 1194.86 | R¬≤: 0.4825 | MAPE: 46.11%\n",
      "Epoch 1990 | Loss: 1152318.3750 | RMSE: 1274.83 | R¬≤: 0.4109 | MAPE: 45.57%\n",
      "‚úÖ Submission saved to: submission.csv\n",
      "üì¶ Best model weights saved to: best_model.pt\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_predict_final(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    test_final=test[predictors],\n",
    "    test_ids=test[['Item_Identifier', 'Outlet_Identifier']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3e768101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d90de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3389cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7835d8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.BatchNorm1d(4096),     # ‚úÖ Normalize after first big projection\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(4096, 2048),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(768, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------- Training + Prediction ----------------------\n",
    "def evaluate_and_predict_final(X_train, X_test, y_train, y_test, test_final, test_ids, submission_file=\"submission.csv\", model_path=\"best_model.pt\"):\n",
    "    \"\"\"\n",
    "    Trains PyTorch MLP on X_train/y_train, evaluates on X_test/y_test.\n",
    "    Saves best model (lowest RMSE) to model_path.\n",
    "    Predicts on test_final and saves submission to CSV with test_ids.\n",
    "    \"\"\"\n",
    "    # Ensure numpy\n",
    "    X_train = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "    X_test = X_test.values if isinstance(X_test, pd.DataFrame) else X_test\n",
    "    y_train = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "    y_test = y_test.values if isinstance(y_test, pd.Series) else y_test\n",
    "    test_final = test_final.values if isinstance(test_final, pd.DataFrame) else test_final\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    model = TorchMLP(X_train.shape[1]).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    print(\"üî• Training PyTorch MLP...\")\n",
    "    for epoch in range(1, 2000):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_preds = model(X_test_tensor).cpu().numpy().flatten()\n",
    "                val_true = y_test_tensor.cpu().numpy().flatten()\n",
    "                rmse = mean_squared_error(val_true, val_preds, squared=False)\n",
    "                r2 = r2_score(val_true, val_preds)\n",
    "                mape = mean_absolute_percentage_error(val_true, val_preds) * 100\n",
    "\n",
    "                print(f\"Epoch {epoch:04d} | Loss: {loss.item():.4f} | RMSE: {rmse:.2f} | R¬≤: {r2:.4f} | MAPE: {mape:.2f}%\")\n",
    "\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse = rmse\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    print(f\"üíæ Best model saved at epoch {epoch} (RMSE: {rmse:.2f})\")\n",
    "\n",
    "    # üîÅ Reload best model before final prediction\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    X_final_tensor = torch.tensor(test_final, dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        final_preds = model(X_final_tensor).cpu().numpy().flatten()\n",
    "        final_preds = np.clip(final_preds, 0, None)\n",
    "\n",
    "    # Save submission\n",
    "    submission = pd.DataFrame({\n",
    "        'Item_Identifier': test_ids['Item_Identifier'].values,\n",
    "        'Outlet_Identifier': test_ids['Outlet_Identifier'].values,\n",
    "        'Item_Outlet_Sales': final_preds\n",
    "    })\n",
    "\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"‚úÖ Submission saved to: {submission_file}\")\n",
    "    print(f\"üì¶ Best model weights saved to: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c848ede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Training PyTorch MLP...\n",
      "Epoch 0001 | Loss: 7843218.5000 | RMSE: 2673.67 | R¬≤: -1.5910 | MAPE: 99.99%\n",
      "üíæ Best model saved at epoch 1 (RMSE: 2673.67)\n",
      "Epoch 0010 | Loss: 6886391.0000 | RMSE: 2043.96 | R¬≤: -0.5142 | MAPE: 90.02%\n",
      "üíæ Best model saved at epoch 10 (RMSE: 2043.96)\n",
      "Epoch 0020 | Loss: 3607988.7500 | RMSE: 1620.59 | R¬≤: 0.0481 | MAPE: 123.34%\n",
      "üíæ Best model saved at epoch 20 (RMSE: 1620.59)\n",
      "Epoch 0030 | Loss: 3212147.7500 | RMSE: 1702.32 | R¬≤: -0.0504 | MAPE: 108.81%\n",
      "Epoch 0040 | Loss: 2760063.0000 | RMSE: 1675.18 | R¬≤: -0.0171 | MAPE: 137.62%\n",
      "Epoch 0050 | Loss: 2488994.0000 | RMSE: 1556.02 | R¬≤: 0.1224 | MAPE: 142.87%\n",
      "üíæ Best model saved at epoch 50 (RMSE: 1556.02)\n",
      "Epoch 0060 | Loss: 2260628.7500 | RMSE: 1473.58 | R¬≤: 0.2130 | MAPE: 123.85%\n",
      "üíæ Best model saved at epoch 60 (RMSE: 1473.58)\n",
      "Epoch 0070 | Loss: 2068158.0000 | RMSE: 1413.29 | R¬≤: 0.2760 | MAPE: 113.70%\n",
      "üíæ Best model saved at epoch 70 (RMSE: 1413.29)\n",
      "Epoch 0080 | Loss: 1877611.7500 | RMSE: 1330.54 | R¬≤: 0.3583 | MAPE: 102.72%\n",
      "üíæ Best model saved at epoch 80 (RMSE: 1330.54)\n",
      "Epoch 0090 | Loss: 1660670.7500 | RMSE: 1247.39 | R¬≤: 0.4360 | MAPE: 94.67%\n",
      "üíæ Best model saved at epoch 90 (RMSE: 1247.39)\n",
      "Epoch 0100 | Loss: 1426966.2500 | RMSE: 1174.73 | R¬≤: 0.4998 | MAPE: 78.53%\n",
      "üíæ Best model saved at epoch 100 (RMSE: 1174.73)\n",
      "Epoch 0110 | Loss: 1306368.0000 | RMSE: 1118.57 | R¬≤: 0.5465 | MAPE: 67.31%\n",
      "üíæ Best model saved at epoch 110 (RMSE: 1118.57)\n",
      "Epoch 0120 | Loss: 1262548.1250 | RMSE: 1128.18 | R¬≤: 0.5387 | MAPE: 67.65%\n",
      "Epoch 0130 | Loss: 1241723.0000 | RMSE: 1099.05 | R¬≤: 0.5622 | MAPE: 61.77%\n",
      "üíæ Best model saved at epoch 130 (RMSE: 1099.05)\n",
      "Epoch 0140 | Loss: 1226845.2500 | RMSE: 1079.27 | R¬≤: 0.5778 | MAPE: 60.27%\n",
      "üíæ Best model saved at epoch 140 (RMSE: 1079.27)\n",
      "Epoch 0150 | Loss: 1213523.6250 | RMSE: 1057.62 | R¬≤: 0.5946 | MAPE: 55.61%\n",
      "üíæ Best model saved at epoch 150 (RMSE: 1057.62)\n",
      "Epoch 0160 | Loss: 1213691.5000 | RMSE: 1050.99 | R¬≤: 0.5996 | MAPE: 52.99%\n",
      "üíæ Best model saved at epoch 160 (RMSE: 1050.99)\n",
      "Epoch 0170 | Loss: 1206460.3750 | RMSE: 1048.07 | R¬≤: 0.6019 | MAPE: 53.17%\n",
      "üíæ Best model saved at epoch 170 (RMSE: 1048.07)\n",
      "Epoch 0180 | Loss: 1195192.7500 | RMSE: 1047.26 | R¬≤: 0.6025 | MAPE: 56.70%\n",
      "üíæ Best model saved at epoch 180 (RMSE: 1047.26)\n",
      "Epoch 0190 | Loss: 1200267.3750 | RMSE: 1046.11 | R¬≤: 0.6033 | MAPE: 52.29%\n",
      "üíæ Best model saved at epoch 190 (RMSE: 1046.11)\n",
      "Epoch 0200 | Loss: 1190733.2500 | RMSE: 1042.09 | R¬≤: 0.6064 | MAPE: 55.75%\n",
      "üíæ Best model saved at epoch 200 (RMSE: 1042.09)\n",
      "Epoch 0210 | Loss: 1182708.6250 | RMSE: 1039.96 | R¬≤: 0.6080 | MAPE: 54.37%\n",
      "üíæ Best model saved at epoch 210 (RMSE: 1039.96)\n",
      "Epoch 0220 | Loss: 1184339.5000 | RMSE: 1045.57 | R¬≤: 0.6038 | MAPE: 57.80%\n",
      "Epoch 0230 | Loss: 1186781.8750 | RMSE: 1047.12 | R¬≤: 0.6026 | MAPE: 51.87%\n",
      "Epoch 0240 | Loss: 1170437.6250 | RMSE: 1039.34 | R¬≤: 0.6085 | MAPE: 53.40%\n",
      "üíæ Best model saved at epoch 240 (RMSE: 1039.34)\n",
      "Epoch 0250 | Loss: 1177550.6250 | RMSE: 1040.45 | R¬≤: 0.6076 | MAPE: 56.61%\n",
      "Epoch 0260 | Loss: 1176126.7500 | RMSE: 1045.09 | R¬≤: 0.6041 | MAPE: 58.81%\n",
      "Epoch 0270 | Loss: 1173137.6250 | RMSE: 1038.14 | R¬≤: 0.6094 | MAPE: 57.28%\n",
      "üíæ Best model saved at epoch 270 (RMSE: 1038.14)\n",
      "Epoch 0280 | Loss: 1176302.8750 | RMSE: 1037.28 | R¬≤: 0.6100 | MAPE: 53.77%\n",
      "üíæ Best model saved at epoch 280 (RMSE: 1037.28)\n",
      "Epoch 0290 | Loss: 1173124.1250 | RMSE: 1040.14 | R¬≤: 0.6079 | MAPE: 58.02%\n",
      "Epoch 0300 | Loss: 1176286.2500 | RMSE: 1048.03 | R¬≤: 0.6019 | MAPE: 60.68%\n",
      "Epoch 0310 | Loss: 1172235.7500 | RMSE: 1036.62 | R¬≤: 0.6105 | MAPE: 55.71%\n",
      "üíæ Best model saved at epoch 310 (RMSE: 1036.62)\n",
      "Epoch 0320 | Loss: 1173933.8750 | RMSE: 1043.50 | R¬≤: 0.6053 | MAPE: 58.77%\n",
      "Epoch 0330 | Loss: 1178293.5000 | RMSE: 1037.50 | R¬≤: 0.6098 | MAPE: 53.88%\n",
      "Epoch 0340 | Loss: 1167049.1250 | RMSE: 1040.67 | R¬≤: 0.6075 | MAPE: 57.88%\n",
      "Epoch 0350 | Loss: 1186580.5000 | RMSE: 1056.01 | R¬≤: 0.5958 | MAPE: 62.36%\n",
      "Epoch 0360 | Loss: 1166615.6250 | RMSE: 1041.77 | R¬≤: 0.6066 | MAPE: 59.18%\n",
      "Epoch 0370 | Loss: 1160686.3750 | RMSE: 1039.38 | R¬≤: 0.6084 | MAPE: 56.14%\n",
      "Epoch 0380 | Loss: 1169067.0000 | RMSE: 1037.76 | R¬≤: 0.6097 | MAPE: 57.06%\n",
      "Epoch 0390 | Loss: 1170234.0000 | RMSE: 1037.78 | R¬≤: 0.6096 | MAPE: 53.45%\n",
      "Epoch 0400 | Loss: 1169502.5000 | RMSE: 1041.20 | R¬≤: 0.6071 | MAPE: 57.78%\n",
      "Epoch 0410 | Loss: 1170448.8750 | RMSE: 1047.76 | R¬≤: 0.6021 | MAPE: 59.81%\n",
      "Epoch 0420 | Loss: 1179746.7500 | RMSE: 1042.15 | R¬≤: 0.6063 | MAPE: 51.29%\n",
      "Epoch 0430 | Loss: 1165663.8750 | RMSE: 1039.69 | R¬≤: 0.6082 | MAPE: 56.34%\n",
      "Epoch 0440 | Loss: 1160438.3750 | RMSE: 1042.03 | R¬≤: 0.6064 | MAPE: 57.44%\n",
      "Epoch 0450 | Loss: 1167680.6250 | RMSE: 1037.54 | R¬≤: 0.6098 | MAPE: 54.85%\n",
      "Epoch 0460 | Loss: 1170245.2500 | RMSE: 1037.18 | R¬≤: 0.6101 | MAPE: 54.13%\n",
      "Epoch 0470 | Loss: 1167582.5000 | RMSE: 1037.29 | R¬≤: 0.6100 | MAPE: 55.33%\n",
      "Epoch 0480 | Loss: 1170923.5000 | RMSE: 1036.65 | R¬≤: 0.6105 | MAPE: 54.92%\n",
      "Epoch 0490 | Loss: 1165020.1250 | RMSE: 1038.51 | R¬≤: 0.6091 | MAPE: 52.89%\n",
      "Epoch 0500 | Loss: 1168936.5000 | RMSE: 1039.24 | R¬≤: 0.6085 | MAPE: 56.71%\n",
      "Epoch 0510 | Loss: 1167907.0000 | RMSE: 1052.37 | R¬≤: 0.5986 | MAPE: 61.95%\n",
      "Epoch 0520 | Loss: 1166729.5000 | RMSE: 1039.90 | R¬≤: 0.6080 | MAPE: 57.22%\n",
      "Epoch 0530 | Loss: 1168889.1250 | RMSE: 1036.90 | R¬≤: 0.6103 | MAPE: 54.23%\n",
      "Epoch 0540 | Loss: 1167727.6250 | RMSE: 1046.86 | R¬≤: 0.6028 | MAPE: 58.88%\n",
      "Epoch 0550 | Loss: 1172840.1250 | RMSE: 1039.34 | R¬≤: 0.6085 | MAPE: 52.45%\n",
      "Epoch 0560 | Loss: 1161824.7500 | RMSE: 1039.55 | R¬≤: 0.6083 | MAPE: 57.40%\n",
      "Epoch 0570 | Loss: 1162431.8750 | RMSE: 1037.57 | R¬≤: 0.6098 | MAPE: 55.66%\n",
      "Epoch 0580 | Loss: 1178027.6250 | RMSE: 1037.61 | R¬≤: 0.6098 | MAPE: 53.60%\n",
      "Epoch 0590 | Loss: 1170704.5000 | RMSE: 1042.14 | R¬≤: 0.6064 | MAPE: 57.74%\n",
      "Epoch 0600 | Loss: 1168231.6250 | RMSE: 1037.10 | R¬≤: 0.6102 | MAPE: 54.68%\n",
      "Epoch 0610 | Loss: 1207553.5000 | RMSE: 1090.19 | R¬≤: 0.5692 | MAPE: 70.58%\n",
      "Epoch 0620 | Loss: 1185027.7500 | RMSE: 1056.98 | R¬≤: 0.5951 | MAPE: 63.51%\n",
      "Epoch 0630 | Loss: 1170867.6250 | RMSE: 1047.71 | R¬≤: 0.6021 | MAPE: 60.80%\n",
      "Epoch 0640 | Loss: 1160330.3750 | RMSE: 1047.27 | R¬≤: 0.6025 | MAPE: 58.85%\n",
      "Epoch 0650 | Loss: 1166781.3750 | RMSE: 1037.38 | R¬≤: 0.6099 | MAPE: 55.09%\n",
      "Epoch 0660 | Loss: 1161690.8750 | RMSE: 1037.69 | R¬≤: 0.6097 | MAPE: 54.44%\n",
      "Epoch 0670 | Loss: 1167182.1250 | RMSE: 1038.09 | R¬≤: 0.6094 | MAPE: 56.31%\n",
      "Epoch 0680 | Loss: 1163009.2500 | RMSE: 1041.30 | R¬≤: 0.6070 | MAPE: 58.23%\n",
      "Epoch 0690 | Loss: 1159259.1250 | RMSE: 1038.38 | R¬≤: 0.6092 | MAPE: 53.86%\n",
      "Epoch 0700 | Loss: 1168453.7500 | RMSE: 1038.86 | R¬≤: 0.6088 | MAPE: 53.62%\n",
      "Epoch 0710 | Loss: 1163599.0000 | RMSE: 1040.76 | R¬≤: 0.6074 | MAPE: 57.09%\n",
      "Epoch 0720 | Loss: 1171324.7500 | RMSE: 1039.84 | R¬≤: 0.6081 | MAPE: 58.41%\n",
      "Epoch 0730 | Loss: 1162990.7500 | RMSE: 1044.32 | R¬≤: 0.6047 | MAPE: 59.05%\n",
      "Epoch 0740 | Loss: 1168516.8750 | RMSE: 1039.09 | R¬≤: 0.6087 | MAPE: 52.67%\n",
      "Epoch 0750 | Loss: 1170243.5000 | RMSE: 1047.01 | R¬≤: 0.6027 | MAPE: 59.97%\n",
      "Epoch 0760 | Loss: 1164315.1250 | RMSE: 1037.54 | R¬≤: 0.6098 | MAPE: 55.76%\n",
      "Epoch 0770 | Loss: 1166348.5000 | RMSE: 1039.30 | R¬≤: 0.6085 | MAPE: 57.94%\n",
      "Epoch 0780 | Loss: 1172207.1250 | RMSE: 1039.09 | R¬≤: 0.6087 | MAPE: 53.13%\n",
      "Epoch 0790 | Loss: 1170060.2500 | RMSE: 1046.75 | R¬≤: 0.6029 | MAPE: 58.57%\n",
      "Epoch 0800 | Loss: 1163646.6250 | RMSE: 1039.23 | R¬≤: 0.6085 | MAPE: 56.54%\n",
      "Epoch 0810 | Loss: 1163184.5000 | RMSE: 1042.60 | R¬≤: 0.6060 | MAPE: 59.23%\n",
      "Epoch 0820 | Loss: 1179786.3750 | RMSE: 1052.05 | R¬≤: 0.5988 | MAPE: 62.33%\n",
      "Epoch 0830 | Loss: 1176314.0000 | RMSE: 1039.11 | R¬≤: 0.6086 | MAPE: 53.78%\n",
      "Epoch 0840 | Loss: 1161637.3750 | RMSE: 1038.52 | R¬≤: 0.6091 | MAPE: 55.54%\n",
      "Epoch 0850 | Loss: 1165550.7500 | RMSE: 1042.16 | R¬≤: 0.6063 | MAPE: 57.79%\n",
      "Epoch 0860 | Loss: 1165430.3750 | RMSE: 1040.80 | R¬≤: 0.6074 | MAPE: 52.05%\n",
      "Epoch 0870 | Loss: 1173790.5000 | RMSE: 1057.46 | R¬≤: 0.5947 | MAPE: 63.02%\n",
      "Epoch 0880 | Loss: 1174800.2500 | RMSE: 1039.38 | R¬≤: 0.6084 | MAPE: 52.59%\n",
      "Epoch 0890 | Loss: 1163596.5000 | RMSE: 1042.80 | R¬≤: 0.6059 | MAPE: 51.34%\n",
      "Epoch 0900 | Loss: 1168350.5000 | RMSE: 1039.76 | R¬≤: 0.6082 | MAPE: 56.61%\n",
      "Epoch 0910 | Loss: 1162005.5000 | RMSE: 1040.21 | R¬≤: 0.6078 | MAPE: 56.73%\n",
      "Epoch 0920 | Loss: 1172932.7500 | RMSE: 1041.16 | R¬≤: 0.6071 | MAPE: 58.02%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0930 | Loss: 1163257.7500 | RMSE: 1038.51 | R¬≤: 0.6091 | MAPE: 55.83%\n",
      "Epoch 0940 | Loss: 1173487.2500 | RMSE: 1052.81 | R¬≤: 0.5983 | MAPE: 62.21%\n",
      "Epoch 0950 | Loss: 1166030.2500 | RMSE: 1045.57 | R¬≤: 0.6038 | MAPE: 50.72%\n",
      "Epoch 0960 | Loss: 1159492.8750 | RMSE: 1039.53 | R¬≤: 0.6083 | MAPE: 56.48%\n",
      "Epoch 0970 | Loss: 1156317.1250 | RMSE: 1040.10 | R¬≤: 0.6079 | MAPE: 56.52%\n",
      "Epoch 0980 | Loss: 1179167.7500 | RMSE: 1066.87 | R¬≤: 0.5874 | MAPE: 65.82%\n",
      "Epoch 0990 | Loss: 1168699.8750 | RMSE: 1044.20 | R¬≤: 0.6048 | MAPE: 51.74%\n",
      "Epoch 1000 | Loss: 1167456.6250 | RMSE: 1039.63 | R¬≤: 0.6083 | MAPE: 55.64%\n",
      "Epoch 1010 | Loss: 1176913.5000 | RMSE: 1063.89 | R¬≤: 0.5898 | MAPE: 64.19%\n",
      "Epoch 1020 | Loss: 1173449.2500 | RMSE: 1047.58 | R¬≤: 0.6022 | MAPE: 50.94%\n",
      "Epoch 1030 | Loss: 1162018.1250 | RMSE: 1041.04 | R¬≤: 0.6072 | MAPE: 57.21%\n",
      "Epoch 1040 | Loss: 1156514.2500 | RMSE: 1038.90 | R¬≤: 0.6088 | MAPE: 55.38%\n",
      "Epoch 1050 | Loss: 1155027.3750 | RMSE: 1042.71 | R¬≤: 0.6059 | MAPE: 56.66%\n",
      "Epoch 1060 | Loss: 1192737.0000 | RMSE: 1063.90 | R¬≤: 0.5897 | MAPE: 66.66%\n",
      "Epoch 1070 | Loss: 1173168.5000 | RMSE: 1040.32 | R¬≤: 0.6077 | MAPE: 55.77%\n",
      "Epoch 1080 | Loss: 1175367.5000 | RMSE: 1044.12 | R¬≤: 0.6049 | MAPE: 51.54%\n",
      "Epoch 1090 | Loss: 1163241.1250 | RMSE: 1038.96 | R¬≤: 0.6088 | MAPE: 53.60%\n",
      "Epoch 1100 | Loss: 1163328.8750 | RMSE: 1039.15 | R¬≤: 0.6086 | MAPE: 52.68%\n",
      "Epoch 1110 | Loss: 1160819.2500 | RMSE: 1042.06 | R¬≤: 0.6064 | MAPE: 57.67%\n",
      "Epoch 1120 | Loss: 1160666.5000 | RMSE: 1038.58 | R¬≤: 0.6090 | MAPE: 53.81%\n",
      "Epoch 1130 | Loss: 1160748.3750 | RMSE: 1041.81 | R¬≤: 0.6066 | MAPE: 57.26%\n",
      "Epoch 1140 | Loss: 1165163.3750 | RMSE: 1047.91 | R¬≤: 0.6020 | MAPE: 60.58%\n",
      "Epoch 1150 | Loss: 1162030.1250 | RMSE: 1039.42 | R¬≤: 0.6084 | MAPE: 52.62%\n",
      "Epoch 1160 | Loss: 1158521.8750 | RMSE: 1046.10 | R¬≤: 0.6034 | MAPE: 58.76%\n",
      "Epoch 1170 | Loss: 1166933.3750 | RMSE: 1048.60 | R¬≤: 0.6015 | MAPE: 50.11%\n",
      "Epoch 1180 | Loss: 1178575.1250 | RMSE: 1049.24 | R¬≤: 0.6010 | MAPE: 60.65%\n",
      "Epoch 1190 | Loss: 1165082.5000 | RMSE: 1039.32 | R¬≤: 0.6085 | MAPE: 53.66%\n",
      "Epoch 1200 | Loss: 1163229.5000 | RMSE: 1041.52 | R¬≤: 0.6068 | MAPE: 57.40%\n",
      "Epoch 1210 | Loss: 1153735.2500 | RMSE: 1038.77 | R¬≤: 0.6089 | MAPE: 54.35%\n",
      "Epoch 1220 | Loss: 1159361.0000 | RMSE: 1040.26 | R¬≤: 0.6078 | MAPE: 53.02%\n",
      "Epoch 1230 | Loss: 1178535.0000 | RMSE: 1044.34 | R¬≤: 0.6047 | MAPE: 51.71%\n",
      "Epoch 1240 | Loss: 1175285.0000 | RMSE: 1058.25 | R¬≤: 0.5941 | MAPE: 62.26%\n",
      "Epoch 1250 | Loss: 1165374.3750 | RMSE: 1039.16 | R¬≤: 0.6086 | MAPE: 52.73%\n",
      "Epoch 1260 | Loss: 1157852.8750 | RMSE: 1041.66 | R¬≤: 0.6067 | MAPE: 56.87%\n",
      "Epoch 1270 | Loss: 1164707.2500 | RMSE: 1042.57 | R¬≤: 0.6060 | MAPE: 59.09%\n",
      "Epoch 1280 | Loss: 1162493.2500 | RMSE: 1039.13 | R¬≤: 0.6086 | MAPE: 55.02%\n",
      "Epoch 1290 | Loss: 1167146.1250 | RMSE: 1046.03 | R¬≤: 0.6034 | MAPE: 50.78%\n",
      "Epoch 1300 | Loss: 1171993.3750 | RMSE: 1052.54 | R¬≤: 0.5985 | MAPE: 61.06%\n",
      "Epoch 1310 | Loss: 1168335.1250 | RMSE: 1040.60 | R¬≤: 0.6075 | MAPE: 52.70%\n",
      "Epoch 1320 | Loss: 1166328.3750 | RMSE: 1048.36 | R¬≤: 0.6016 | MAPE: 60.02%\n",
      "Epoch 1330 | Loss: 1159704.5000 | RMSE: 1039.64 | R¬≤: 0.6082 | MAPE: 53.64%\n",
      "Epoch 1340 | Loss: 1157474.5000 | RMSE: 1042.82 | R¬≤: 0.6058 | MAPE: 57.05%\n",
      "Epoch 1350 | Loss: 1195465.1250 | RMSE: 1055.72 | R¬≤: 0.5960 | MAPE: 65.87%\n",
      "Epoch 1360 | Loss: 1172490.0000 | RMSE: 1060.44 | R¬≤: 0.5924 | MAPE: 61.17%\n",
      "Epoch 1370 | Loss: 1158319.2500 | RMSE: 1038.97 | R¬≤: 0.6087 | MAPE: 55.44%\n",
      "Epoch 1380 | Loss: 1161731.5000 | RMSE: 1039.24 | R¬≤: 0.6085 | MAPE: 54.75%\n",
      "Epoch 1390 | Loss: 1166977.0000 | RMSE: 1047.71 | R¬≤: 0.6021 | MAPE: 58.32%\n",
      "Epoch 1400 | Loss: 1156084.3750 | RMSE: 1038.99 | R¬≤: 0.6087 | MAPE: 53.95%\n",
      "Epoch 1410 | Loss: 1166495.6250 | RMSE: 1048.97 | R¬≤: 0.6012 | MAPE: 60.79%\n",
      "Epoch 1420 | Loss: 1159307.6250 | RMSE: 1041.18 | R¬≤: 0.6071 | MAPE: 52.67%\n",
      "Epoch 1430 | Loss: 1160390.1250 | RMSE: 1041.07 | R¬≤: 0.6072 | MAPE: 57.28%\n",
      "Epoch 1440 | Loss: 1160776.0000 | RMSE: 1043.04 | R¬≤: 0.6057 | MAPE: 58.63%\n",
      "Epoch 1450 | Loss: 1156493.7500 | RMSE: 1039.55 | R¬≤: 0.6083 | MAPE: 54.57%\n",
      "Epoch 1460 | Loss: 1158228.8750 | RMSE: 1039.69 | R¬≤: 0.6082 | MAPE: 54.66%\n",
      "Epoch 1470 | Loss: 1157576.3750 | RMSE: 1046.97 | R¬≤: 0.6027 | MAPE: 59.29%\n",
      "Epoch 1480 | Loss: 1170375.5000 | RMSE: 1045.45 | R¬≤: 0.6039 | MAPE: 60.04%\n",
      "Epoch 1490 | Loss: 1168517.2500 | RMSE: 1043.52 | R¬≤: 0.6053 | MAPE: 59.49%\n",
      "Epoch 1500 | Loss: 1173187.3750 | RMSE: 1045.18 | R¬≤: 0.6041 | MAPE: 50.90%\n",
      "Epoch 1510 | Loss: 1153710.0000 | RMSE: 1040.98 | R¬≤: 0.6072 | MAPE: 51.94%\n",
      "Epoch 1520 | Loss: 1155385.8750 | RMSE: 1040.35 | R¬≤: 0.6077 | MAPE: 55.63%\n",
      "Epoch 1530 | Loss: 1160222.2500 | RMSE: 1039.95 | R¬≤: 0.6080 | MAPE: 56.50%\n",
      "Epoch 1540 | Loss: 1157304.1250 | RMSE: 1039.73 | R¬≤: 0.6082 | MAPE: 53.48%\n",
      "Epoch 1550 | Loss: 1170830.0000 | RMSE: 1044.06 | R¬≤: 0.6049 | MAPE: 51.54%\n",
      "Epoch 1560 | Loss: 1158061.2500 | RMSE: 1042.27 | R¬≤: 0.6063 | MAPE: 57.02%\n",
      "Epoch 1570 | Loss: 1170308.8750 | RMSE: 1052.59 | R¬≤: 0.5984 | MAPE: 61.60%\n",
      "Epoch 1580 | Loss: 1157490.1250 | RMSE: 1046.52 | R¬≤: 0.6030 | MAPE: 58.68%\n",
      "Epoch 1590 | Loss: 1177736.5000 | RMSE: 1058.89 | R¬≤: 0.5936 | MAPE: 63.84%\n",
      "Epoch 1600 | Loss: 1182155.7500 | RMSE: 1057.29 | R¬≤: 0.5948 | MAPE: 49.22%\n",
      "Epoch 1610 | Loss: 1167691.3750 | RMSE: 1042.17 | R¬≤: 0.6063 | MAPE: 57.34%\n",
      "Epoch 1620 | Loss: 1169778.3750 | RMSE: 1048.25 | R¬≤: 0.6017 | MAPE: 59.57%\n",
      "Epoch 1630 | Loss: 1157967.5000 | RMSE: 1042.84 | R¬≤: 0.6058 | MAPE: 57.80%\n",
      "Epoch 1640 | Loss: 1163529.7500 | RMSE: 1039.45 | R¬≤: 0.6084 | MAPE: 53.71%\n",
      "Epoch 1650 | Loss: 1156057.3750 | RMSE: 1045.91 | R¬≤: 0.6035 | MAPE: 58.72%\n",
      "Epoch 1660 | Loss: 1163351.7500 | RMSE: 1041.65 | R¬≤: 0.6067 | MAPE: 52.86%\n",
      "Epoch 1670 | Loss: 1161422.0000 | RMSE: 1057.20 | R¬≤: 0.5949 | MAPE: 61.61%\n",
      "Epoch 1680 | Loss: 1165735.2500 | RMSE: 1046.11 | R¬≤: 0.6034 | MAPE: 51.23%\n",
      "Epoch 1690 | Loss: 1172936.0000 | RMSE: 1056.04 | R¬≤: 0.5958 | MAPE: 61.35%\n",
      "Epoch 1700 | Loss: 1161330.1250 | RMSE: 1040.74 | R¬≤: 0.6074 | MAPE: 52.99%\n",
      "Epoch 1710 | Loss: 1156489.7500 | RMSE: 1041.15 | R¬≤: 0.6071 | MAPE: 52.70%\n",
      "Epoch 1720 | Loss: 1155411.7500 | RMSE: 1045.93 | R¬≤: 0.6035 | MAPE: 58.83%\n",
      "Epoch 1730 | Loss: 1155490.7500 | RMSE: 1040.09 | R¬≤: 0.6079 | MAPE: 55.63%\n",
      "Epoch 1740 | Loss: 1163383.8750 | RMSE: 1041.21 | R¬≤: 0.6071 | MAPE: 56.00%\n",
      "Epoch 1750 | Loss: 1177130.6250 | RMSE: 1056.56 | R¬≤: 0.5954 | MAPE: 49.31%\n",
      "Epoch 1760 | Loss: 1204320.0000 | RMSE: 1085.57 | R¬≤: 0.5729 | MAPE: 69.54%\n",
      "Epoch 1770 | Loss: 1158164.2500 | RMSE: 1051.65 | R¬≤: 0.5991 | MAPE: 60.96%\n",
      "Epoch 1780 | Loss: 1160516.2500 | RMSE: 1040.10 | R¬≤: 0.6079 | MAPE: 54.13%\n",
      "Epoch 1790 | Loss: 1160940.3750 | RMSE: 1040.39 | R¬≤: 0.6077 | MAPE: 53.25%\n",
      "Epoch 1800 | Loss: 1160288.0000 | RMSE: 1043.34 | R¬≤: 0.6055 | MAPE: 57.30%\n",
      "Epoch 1810 | Loss: 1152199.0000 | RMSE: 1042.48 | R¬≤: 0.6061 | MAPE: 56.71%\n",
      "Epoch 1820 | Loss: 1160219.3750 | RMSE: 1040.76 | R¬≤: 0.6074 | MAPE: 53.11%\n",
      "Epoch 1830 | Loss: 1159115.7500 | RMSE: 1052.61 | R¬≤: 0.5984 | MAPE: 61.20%\n",
      "Epoch 1840 | Loss: 1165724.3750 | RMSE: 1048.54 | R¬≤: 0.6015 | MAPE: 50.19%\n",
      "Epoch 1850 | Loss: 1162837.1250 | RMSE: 1051.37 | R¬≤: 0.5994 | MAPE: 60.31%\n",
      "Epoch 1860 | Loss: 1163182.0000 | RMSE: 1048.91 | R¬≤: 0.6012 | MAPE: 50.23%\n",
      "Epoch 1870 | Loss: 1192761.3750 | RMSE: 1069.55 | R¬≤: 0.5854 | MAPE: 65.09%\n",
      "Epoch 1880 | Loss: 1168183.1250 | RMSE: 1042.72 | R¬≤: 0.6059 | MAPE: 57.33%\n",
      "Epoch 1890 | Loss: 1159448.0000 | RMSE: 1041.59 | R¬≤: 0.6068 | MAPE: 52.19%\n",
      "Epoch 1900 | Loss: 1149111.6250 | RMSE: 1040.00 | R¬≤: 0.6080 | MAPE: 54.39%\n",
      "Epoch 1910 | Loss: 1157097.0000 | RMSE: 1041.01 | R¬≤: 0.6072 | MAPE: 54.43%\n",
      "Epoch 1920 | Loss: 1157193.7500 | RMSE: 1046.66 | R¬≤: 0.6029 | MAPE: 58.94%\n",
      "Epoch 1930 | Loss: 1155950.0000 | RMSE: 1048.89 | R¬≤: 0.6012 | MAPE: 60.42%\n",
      "Epoch 1940 | Loss: 1159347.1250 | RMSE: 1045.23 | R¬≤: 0.6040 | MAPE: 57.90%\n",
      "Epoch 1950 | Loss: 1158265.0000 | RMSE: 1054.12 | R¬≤: 0.5973 | MAPE: 61.66%\n",
      "Epoch 1960 | Loss: 1161540.8750 | RMSE: 1045.13 | R¬≤: 0.6041 | MAPE: 51.79%\n",
      "Epoch 1970 | Loss: 1188483.8750 | RMSE: 1069.61 | R¬≤: 0.5853 | MAPE: 67.48%\n",
      "Epoch 1980 | Loss: 1152906.2500 | RMSE: 1053.63 | R¬≤: 0.5976 | MAPE: 61.67%\n",
      "Epoch 1990 | Loss: 1160673.6250 | RMSE: 1043.91 | R¬≤: 0.6050 | MAPE: 57.57%\n",
      "‚úÖ Submission saved to: submission.csv\n",
      "üì¶ Best model weights saved to: best_model.pt\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_predict_final(\n",
    "    X_train, X_test, y_train, y_test,\n",
    "    test_final=test[predictors],\n",
    "    test_ids=test[['Item_Identifier', 'Outlet_Identifier']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09e480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
